{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "'''\n",
    "https://arxiv.org/pdf/1608.06993\n",
    "'''\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels=growth_rate, kernel_size=(3, 3), padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d(x)\n",
    "        return x\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=16):\n",
    "        super().__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(6):\n",
    "            self.layers.append(DenseLayer(in_channels + (i * growth_rate), growth_rate))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = x\n",
    "        for layer in self.layers:\n",
    "            new_features = layer(features)\n",
    "            features = torch.cat([features, new_features], dim=1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('bnorm1_1', nn.BatchNorm2d(in_channels)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('conv1_1', nn.Conv2d(in_channels, self.out_channels, kernel_size=(1, 1))),\n",
    "            ('adaptive1_1', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "'''\n",
    "Use smaller version like DenseNet-40: [6, 6, 6] blocks with k=12\n",
    "Initial convolution (7x7 or 3x3 for CIFAR)\n",
    "Dense Block 1 → Transition 1\n",
    "Dense Block 2 → Transition 2\n",
    "Dense Block 3 → Transition 3\n",
    "Dense Block 4 (no transition after last block)\n",
    "Global average pooling\n",
    "Fully connected layer\n",
    "\n",
    "Start with k₀ channels (e.g., 64)\n",
    "\n",
    "Shape tracking:\n",
    "'''    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=(3, 3), stride=2),\n",
    "            DenseBlock(128),\n",
    "            TransitionLayer(224, 112),\n",
    "            DenseBlock(112),\n",
    "            TransitionLayer(208, 104),\n",
    "            DenseBlock(104),\n",
    "            TransitionLayer(200, 100),     \n",
    "            DenseBlock(100),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.classifier = nn.Linear(196, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x) # shape is [batch, 98, 1, 1]\n",
    "        x = x.view(x.size(0), -1) # shape is [batch, 98]\n",
    "        x = self.classifier(x) # output is [batch, 100]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af65b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n",
      "Starting Epoch 1\n",
      "Batch 0/44, Loss: 4.6182\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.3794\n",
      "Starting Epoch 2\n",
      "Batch 0/44, Loss: 4.1531\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0497\n",
      "Epoch 2 finished\n",
      "average training loss is 4.0497\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0497\n",
      "Validation - Loss: 3.8909\n",
      "Starting Epoch 3\n",
      "Batch 0/44, Loss: 3.9330\n",
      "Epoch 3 finished\n",
      "Training - Loss: 3.8483\n",
      "Starting Epoch 4\n",
      "Batch 0/44, Loss: 3.7237\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.6663\n",
      "Epoch 4 finished\n",
      "average training loss is 3.6663\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.6663\n",
      "Validation - Loss: 3.5365\n",
      "Starting Epoch 5\n",
      "Batch 0/44, Loss: 3.6231\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.5005\n",
      "Starting Epoch 6\n",
      "Batch 0/44, Loss: 3.3913\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.3758\n",
      "Epoch 6 finished\n",
      "average training loss is 3.3758\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.3758\n",
      "Validation - Loss: 3.3541\n",
      "Starting Epoch 7\n",
      "Batch 0/44, Loss: 3.3047\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.2584\n",
      "Starting Epoch 8\n",
      "Batch 0/44, Loss: 3.2231\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.1842\n",
      "Epoch 8 finished\n",
      "average training loss is 3.1842\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.1842\n",
      "Validation - Loss: 3.1418\n",
      "Starting Epoch 9\n",
      "Batch 0/44, Loss: 3.1307\n",
      "Epoch 9 finished\n",
      "Training - Loss: 3.0938\n",
      "Starting Epoch 10\n",
      "Batch 0/44, Loss: 3.0422\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0139\n",
      "Epoch 10 finished\n",
      "average training loss is 3.0139\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0139\n",
      "Validation - Loss: 3.0764\n",
      "Starting Epoch 11\n",
      "Batch 0/44, Loss: 3.0830\n",
      "Epoch 11 finished\n",
      "Training - Loss: 2.9485\n",
      "Starting Epoch 12\n",
      "Batch 0/44, Loss: 2.8950\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.8975\n",
      "Epoch 12 finished\n",
      "average training loss is 2.8975\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.8975\n",
      "Validation - Loss: 2.9182\n",
      "Starting Epoch 13\n",
      "Batch 0/44, Loss: 2.8581\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.8404\n",
      "Starting Epoch 14\n",
      "Batch 0/44, Loss: 2.9227\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.7899\n",
      "Epoch 14 finished\n",
      "average training loss is 2.7899\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.7899\n",
      "Validation - Loss: 2.7988\n",
      "Starting Epoch 15\n",
      "Batch 0/44, Loss: 2.7271\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.7288\n",
      "Starting Epoch 16\n",
      "Batch 0/44, Loss: 2.7392\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.6827\n",
      "Epoch 16 finished\n",
      "average training loss is 2.6827\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.6827\n",
      "Validation - Loss: 2.8064\n",
      "Starting Epoch 17\n",
      "Batch 0/44, Loss: 2.5969\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.6222\n",
      "Starting Epoch 18\n",
      "Batch 0/44, Loss: 2.5993\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.5955\n",
      "Epoch 18 finished\n",
      "average training loss is 2.5955\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.5955\n",
      "Validation - Loss: 2.5421\n",
      "Starting Epoch 19\n",
      "Batch 0/44, Loss: 2.5160\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.5393\n",
      "Starting Epoch 20\n",
      "Batch 0/44, Loss: 2.4892\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5159\n",
      "Epoch 20 finished\n",
      "average training loss is 2.5159\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5159\n",
      "Validation - Loss: 2.5150\n",
      "Starting Epoch 21\n",
      "Batch 0/44, Loss: 2.4978\n",
      "Epoch 21 finished\n",
      "Training - Loss: 2.4660\n",
      "Starting Epoch 22\n",
      "Batch 0/44, Loss: 2.4264\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.4357\n",
      "Epoch 22 finished\n",
      "average training loss is 2.4357\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.4357\n",
      "Validation - Loss: 2.4281\n",
      "Starting Epoch 23\n",
      "Batch 0/44, Loss: 2.3884\n",
      "Epoch 23 finished\n",
      "Training - Loss: 2.3946\n",
      "Starting Epoch 24\n",
      "Batch 0/44, Loss: 2.3502\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.3590\n",
      "Epoch 24 finished\n",
      "average training loss is 2.3590\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.3590\n",
      "Validation - Loss: 2.3819\n",
      "Starting Epoch 25\n",
      "Batch 0/44, Loss: 2.2349\n",
      "Epoch 25 finished\n",
      "Training - Loss: 2.3328\n",
      "Starting Epoch 26\n",
      "Batch 0/44, Loss: 2.2522\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.2902\n",
      "Epoch 26 finished\n",
      "average training loss is 2.2902\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.2902\n",
      "Validation - Loss: 2.3190\n",
      "Starting Epoch 27\n",
      "Batch 0/44, Loss: 2.1978\n",
      "Epoch 27 finished\n",
      "Training - Loss: 2.2690\n",
      "Starting Epoch 28\n",
      "Batch 0/44, Loss: 2.2381\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.2380\n",
      "Epoch 28 finished\n",
      "average training loss is 2.2380\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.2380\n",
      "Validation - Loss: 2.2250\n",
      "Starting Epoch 29\n",
      "Batch 0/44, Loss: 2.2575\n",
      "Epoch 29 finished\n",
      "Training - Loss: 2.2036\n",
      "Starting Epoch 30\n",
      "Batch 0/44, Loss: 2.1429\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.1714\n",
      "Epoch 30 finished\n",
      "average training loss is 2.1714\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.1714\n",
      "Validation - Loss: 2.2134\n",
      "Starting Epoch 31\n",
      "Batch 0/44, Loss: 2.1705\n",
      "Epoch 31 finished\n",
      "Training - Loss: 2.1411\n",
      "Starting Epoch 32\n",
      "Batch 0/44, Loss: 2.1011\n",
      "Epoch 32 finished\n",
      "Training - Loss: 2.1169\n",
      "Epoch 32 finished\n",
      "average training loss is 2.1169\n",
      "Epoch 32 finished\n",
      "Training - Loss: 2.1169\n",
      "Validation - Loss: 2.1499\n",
      "Starting Epoch 33\n",
      "Batch 0/44, Loss: 2.1272\n",
      "Epoch 33 finished\n",
      "Training - Loss: 2.0901\n",
      "Starting Epoch 34\n",
      "Batch 0/44, Loss: 2.0737\n",
      "Epoch 34 finished\n",
      "Training - Loss: 2.0704\n",
      "Epoch 34 finished\n",
      "average training loss is 2.0704\n",
      "Epoch 34 finished\n",
      "Training - Loss: 2.0704\n",
      "Validation - Loss: 2.1161\n",
      "Starting Epoch 35\n",
      "Batch 0/44, Loss: 2.0385\n",
      "Epoch 35 finished\n",
      "Training - Loss: 2.0504\n",
      "Starting Epoch 36\n",
      "Batch 0/44, Loss: 2.0857\n",
      "Epoch 36 finished\n",
      "Training - Loss: 2.0336\n",
      "Epoch 36 finished\n",
      "average training loss is 2.0336\n",
      "Epoch 36 finished\n",
      "Training - Loss: 2.0336\n",
      "Validation - Loss: 2.1048\n",
      "Starting Epoch 37\n",
      "Batch 0/44, Loss: 2.0459\n",
      "Epoch 37 finished\n",
      "Training - Loss: 2.0265\n",
      "Starting Epoch 38\n",
      "Batch 0/44, Loss: 1.9810\n",
      "Epoch 38 finished\n",
      "Training - Loss: 2.0150\n",
      "Epoch 38 finished\n",
      "average training loss is 2.0150\n",
      "Epoch 38 finished\n",
      "Training - Loss: 2.0150\n",
      "Validation - Loss: 2.0850\n",
      "Starting Epoch 39\n",
      "Batch 0/44, Loss: 1.9805\n",
      "Epoch 39 finished\n",
      "Training - Loss: 2.0075\n",
      "Starting Epoch 40\n",
      "Batch 0/44, Loss: 2.0239\n",
      "Epoch 40 finished\n",
      "Training - Loss: 2.0035\n",
      "Epoch 40 finished\n",
      "average training loss is 2.0035\n",
      "Epoch 40 finished\n",
      "Training - Loss: 2.0035\n",
      "Validation - Loss: 2.0833\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.5)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])   \n",
    "\n",
    "cifar_train_raw = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))\n",
    "\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "denseNet = DenseNet().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    denseNet.parameters(),\n",
    "    lr=3e-3,  # Keep this for now, let OneCycleLR handle it\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-3,                # Slightly lower peak LR for stability\n",
    "    epochs=num_epochs,          # Keep 45 epochs\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.4,              # Increase warmup to 40% (18 epochs)\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=12.0,            # Start LR = 5e-3 / 12 = 4.2e-4\n",
    "    final_div_factor=400.0      # Final LR = 5e-3 / 400 = 1.25e-5\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    denseNet.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = denseNet(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        denseNet.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = denseNet(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a6de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 69.94%\n",
      "Standard Test Accuracy: 69.9440 (6994.40%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(denseNet)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c7556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
