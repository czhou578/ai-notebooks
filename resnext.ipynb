{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfcc9a1",
   "metadata": {},
   "source": [
    "paper link: https://arxiv.org/pdf/1611.05431\n",
    "\n",
    "# Deep Residual Learning for Image Recognition\n",
    "\n",
    "Our method indicates that cardinality (the size of the\n",
    "set of transformations) is a concrete, measurable dimension that is of central importance, in addition to the dimensions of width and depth. Experiments demonstrate that increasing cardinality is a more effective way of gaining accuracy than going deeper or wider, especially when depth and\n",
    "width starts to give diminishing returns for existing models.\n",
    "\n",
    "(i) if producing spatial maps of the same size, the blocks share\n",
    "the same hyper-parameters (width and filter sizes), and \n",
    "\n",
    "(ii) each time when the spatial map is downsampled by a factor of 2, the width of the blocks is multiplied by a factor\n",
    "of 2.\n",
    "\n",
    "Arbitrary function that projects into a lower dimensional embedding space and then transforms it. Each transformation is of the same topology (homogeneous). The outputs of the transformations are aggregated by summation.\n",
    "\n",
    "More modularized design \n",
    "Reduced hyper-parameters\n",
    "Cardinality: representing the number of independent transformation paths to be aggregated\n",
    "\n",
    "If building a neural network is like organizing a team to solve a complex problem, depth is the number of management layers a decision must pass through, and width is the size of the individual departments. Cardinality is like splitting the problem into many small, identical task forces that work in parallel. Instead of just hiring more people for one giant department (width) or adding more bosses (depth), cardinality proves that having many specialized, parallel paths working on low-dimensional versions of the problem is a more efficient way to reach the correct solution.\n",
    "\n",
    "Analogy for Aggregated Transformations Imagine you are a head chef (the neuron) tasked with creating a complex sauce. A simple neuron is like you doing every step yourself: chopping one ingredient, then the next, then stirring them all in a single pot. Aggregated transformation is like hiring a team of 32 specialized sous-chefs (cardinality). You give each chef the same recipe (homogeneous topology), but they each work on a small, distinct portion of the ingredients in their own pans (split and transform). Finally, you pour all their individual results into the main pot (aggregate) to create a sauce that is far more complex and refined than what you could have achieved alone in the same amount of time.\n",
    "\n",
    "Downsampling is performed by the first convolutional layer in the block, with a stride of 2. When the feature map size is halved, the number of output channels is doubled to preserve the computational complexity per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212b47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We train the models on the 50k training set and evaluate\n",
    "on the 10k test set. The input image is 32×32 randomly\n",
    "cropped from a zero-padded 40×40 image or its flipping,\n",
    "following [14]. No other data augmentation is used. The\n",
    "first layer is 3×3 conv with 64 filters. There are 3 stages\n",
    "each having 3 residual blocks, and the output map size is\n",
    "32, 16, and 8 for each stage [14]. The network ends with a\n",
    "global average pooling and a fully-connected layer. Width\n",
    "is increased by 2× when the stage changes (downsampling),\n",
    "as in Sec. 3.1.\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class ResNeXtModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, reduce_channel, cardinality, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        assert reduce_channel % cardinality == 0, \\\n",
    "            f\"bottleneck_channels ({reduce_channel}) must be divisible by cardinality ({cardinality})\"\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, reduce_channel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(reduce_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(reduce_channel, reduce_channel, kernel_size=3, stride=stride,\n",
    "                     padding=1, groups=cardinality, bias=False),\n",
    "            nn.BatchNorm2d(reduce_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(reduce_channel, out_channel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "        )\n",
    "\n",
    "        # Shortcut connection for dimension matching\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channel != out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channel)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.layers(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNeXtCIFAR(nn.Module):\n",
    "    def __init__(self, cardinality=8, width=64, num_classes=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First layer: 3×3 conv with 64 filters (no stride, no pooling for CIFAR)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Stage 1: 3 blocks, output map size 32×32, width=64\n",
    "        self.stage1 = self._make_stage(64, 64, width, cardinality, 3, stride=1)\n",
    "        \n",
    "        # Stage 2: 3 blocks, output map size 16×16, width=128 (2× increase)\n",
    "        self.stage2 = self._make_stage(64, 128, width*2, cardinality, 3, stride=2)\n",
    "        \n",
    "        # Stage 3: 3 blocks, output map size 8×8, width=256 (2× increase)\n",
    "        self.stage3 = self._make_stage(128, 256, width*4, cardinality, 3, stride=2)\n",
    "        \n",
    "        # Global average pooling + fully connected\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def _make_stage(self, in_channels, out_channels, reduce_channels, cardinality, num_blocks, stride):\n",
    "        \"\"\"Create a stage with multiple ResNeXt blocks\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        # First block handles downsampling and channel increase\n",
    "        layers.append(ResNeXtModule(in_channels, out_channels, reduce_channels, \n",
    "                                   cardinality, stride=stride))\n",
    "        \n",
    "        # Remaining blocks keep same dimensions\n",
    "        for _ in range(num_blocks - 1):\n",
    "            layers.append(ResNeXtModule(out_channels, out_channels, reduce_channels, \n",
    "                                       cardinality, stride=1))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)      # 32×32×64\n",
    "        x = self.stage1(x)     # 32×32×64\n",
    "        x = self.stage2(x)     # 16×16×128\n",
    "        x = self.stage3(x)     # 8×8×256\n",
    "        x = self.avgpool(x)    # 1×1×256\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b71b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "100%|██████████| 169M/169M [00:04<00:00, 35.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Batch 0/44, Loss: 4.8192\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.4015\n",
      "Starting Epoch 2\n",
      "Batch 0/44, Loss: 4.1460\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0335\n",
      "Epoch 2 finished\n",
      "average training loss is 4.0335\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0335\n",
      "Validation - Loss: 3.9083\n",
      "Starting Epoch 3\n",
      "Batch 0/44, Loss: 3.8987\n",
      "Epoch 3 finished\n",
      "Training - Loss: 3.8083\n",
      "Starting Epoch 4\n",
      "Batch 0/44, Loss: 3.7351\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.5647\n",
      "Epoch 4 finished\n",
      "average training loss is 3.5647\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.5647\n",
      "Validation - Loss: 3.9766\n",
      "Starting Epoch 5\n",
      "Batch 0/44, Loss: 3.4383\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.3191\n",
      "Starting Epoch 6\n",
      "Batch 0/44, Loss: 3.1680\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.0940\n",
      "Epoch 6 finished\n",
      "average training loss is 3.0940\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.0940\n",
      "Validation - Loss: 3.7334\n",
      "Starting Epoch 7\n",
      "Batch 0/44, Loss: 3.0051\n",
      "Epoch 7 finished\n",
      "Training - Loss: 2.9239\n",
      "Starting Epoch 8\n",
      "Batch 0/44, Loss: 2.8587\n",
      "Epoch 8 finished\n",
      "Training - Loss: 2.7561\n",
      "Epoch 8 finished\n",
      "average training loss is 2.7561\n",
      "Epoch 8 finished\n",
      "Training - Loss: 2.7561\n",
      "Validation - Loss: 3.5028\n",
      "Starting Epoch 9\n",
      "Batch 0/44, Loss: 2.7153\n",
      "Epoch 9 finished\n",
      "Training - Loss: 2.6362\n",
      "Starting Epoch 10\n",
      "Batch 0/44, Loss: 2.5357\n",
      "Epoch 10 finished\n",
      "Training - Loss: 2.5204\n",
      "Epoch 10 finished\n",
      "average training loss is 2.5204\n",
      "Epoch 10 finished\n",
      "Training - Loss: 2.5204\n",
      "Validation - Loss: 2.7939\n",
      "Starting Epoch 11\n",
      "Batch 0/44, Loss: 2.4963\n",
      "Epoch 11 finished\n",
      "Training - Loss: 2.4320\n",
      "Starting Epoch 12\n",
      "Batch 0/44, Loss: 2.2650\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.3378\n",
      "Epoch 12 finished\n",
      "average training loss is 2.3378\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.3378\n",
      "Validation - Loss: 2.6901\n",
      "Starting Epoch 13\n",
      "Batch 0/44, Loss: 2.3079\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.2386\n",
      "Starting Epoch 14\n",
      "Batch 0/44, Loss: 2.1697\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.1716\n",
      "Epoch 14 finished\n",
      "average training loss is 2.1716\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.1716\n",
      "Validation - Loss: 2.5800\n",
      "Starting Epoch 15\n",
      "Batch 0/44, Loss: 2.0437\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.1097\n",
      "Starting Epoch 16\n",
      "Batch 0/44, Loss: 2.0116\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.0601\n",
      "Epoch 16 finished\n",
      "average training loss is 2.0601\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.0601\n",
      "Validation - Loss: 2.3914\n",
      "Starting Epoch 17\n",
      "Batch 0/44, Loss: 1.9927\n",
      "Epoch 17 finished\n",
      "Training - Loss: 1.9956\n",
      "Starting Epoch 18\n",
      "Batch 0/44, Loss: 1.9612\n",
      "Epoch 18 finished\n",
      "Training - Loss: 1.9439\n",
      "Epoch 18 finished\n",
      "average training loss is 1.9439\n",
      "Epoch 18 finished\n",
      "Training - Loss: 1.9439\n",
      "Validation - Loss: 2.4270\n",
      "Starting Epoch 19\n",
      "Batch 0/44, Loss: 1.9466\n",
      "Epoch 19 finished\n",
      "Training - Loss: 1.8963\n",
      "Starting Epoch 20\n",
      "Batch 0/44, Loss: 1.8874\n",
      "Epoch 20 finished\n",
      "Training - Loss: 1.8542\n",
      "Epoch 20 finished\n",
      "average training loss is 1.8542\n",
      "Epoch 20 finished\n",
      "Training - Loss: 1.8542\n",
      "Validation - Loss: 2.4900\n",
      "Starting Epoch 21\n",
      "Batch 0/44, Loss: 1.7591\n",
      "Epoch 21 finished\n",
      "Training - Loss: 1.8052\n",
      "Starting Epoch 22\n",
      "Batch 0/44, Loss: 1.7435\n",
      "Epoch 22 finished\n",
      "Training - Loss: 1.7675\n",
      "Epoch 22 finished\n",
      "average training loss is 1.7675\n",
      "Epoch 22 finished\n",
      "Training - Loss: 1.7675\n",
      "Validation - Loss: 2.1437\n",
      "Starting Epoch 23\n",
      "Batch 0/44, Loss: 1.6957\n",
      "Epoch 23 finished\n",
      "Training - Loss: 1.7319\n",
      "Starting Epoch 24\n",
      "Batch 0/44, Loss: 1.6598\n",
      "Epoch 24 finished\n",
      "Training - Loss: 1.6798\n",
      "Epoch 24 finished\n",
      "average training loss is 1.6798\n",
      "Epoch 24 finished\n",
      "Training - Loss: 1.6798\n",
      "Validation - Loss: 2.0299\n",
      "Starting Epoch 25\n",
      "Batch 0/44, Loss: 1.5973\n",
      "Epoch 25 finished\n",
      "Training - Loss: 1.6464\n",
      "Starting Epoch 26\n",
      "Batch 0/44, Loss: 1.5815\n",
      "Epoch 26 finished\n",
      "Training - Loss: 1.6043\n",
      "Epoch 26 finished\n",
      "average training loss is 1.6043\n",
      "Epoch 26 finished\n",
      "Training - Loss: 1.6043\n",
      "Validation - Loss: 2.0553\n",
      "Starting Epoch 27\n",
      "Batch 0/44, Loss: 1.5579\n",
      "Epoch 27 finished\n",
      "Training - Loss: 1.5762\n",
      "Starting Epoch 28\n",
      "Batch 0/44, Loss: 1.5223\n",
      "Epoch 28 finished\n",
      "Training - Loss: 1.5300\n",
      "Epoch 28 finished\n",
      "average training loss is 1.5300\n",
      "Epoch 28 finished\n",
      "Training - Loss: 1.5300\n",
      "Validation - Loss: 1.9909\n",
      "Starting Epoch 29\n",
      "Batch 0/44, Loss: 1.4682\n",
      "Epoch 29 finished\n",
      "Training - Loss: 1.4961\n",
      "Starting Epoch 30\n",
      "Batch 0/44, Loss: 1.4262\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.4612\n",
      "Epoch 30 finished\n",
      "average training loss is 1.4612\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.4612\n",
      "Validation - Loss: 1.9432\n",
      "Starting Epoch 31\n",
      "Batch 0/44, Loss: 1.4078\n",
      "Epoch 31 finished\n",
      "Training - Loss: 1.4322\n",
      "Starting Epoch 32\n",
      "Batch 0/44, Loss: 1.4100\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.4045\n",
      "Epoch 32 finished\n",
      "average training loss is 1.4045\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.4045\n",
      "Validation - Loss: 1.8986\n",
      "Starting Epoch 33\n",
      "Batch 0/44, Loss: 1.3714\n",
      "Epoch 33 finished\n",
      "Training - Loss: 1.3770\n",
      "Starting Epoch 34\n",
      "Batch 0/44, Loss: 1.3481\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.3517\n",
      "Epoch 34 finished\n",
      "average training loss is 1.3517\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.3517\n",
      "Validation - Loss: 1.8475\n",
      "Starting Epoch 35\n",
      "Batch 0/44, Loss: 1.3371\n",
      "Epoch 35 finished\n",
      "Training - Loss: 1.3335\n",
      "Starting Epoch 36\n",
      "Batch 0/44, Loss: 1.3172\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.3216\n",
      "Epoch 36 finished\n",
      "average training loss is 1.3216\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.3216\n",
      "Validation - Loss: 1.8388\n",
      "Starting Epoch 37\n",
      "Batch 0/44, Loss: 1.3263\n",
      "Epoch 37 finished\n",
      "Training - Loss: 1.3070\n",
      "Starting Epoch 38\n",
      "Batch 0/44, Loss: 1.3035\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.3031\n",
      "Epoch 38 finished\n",
      "average training loss is 1.3031\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.3031\n",
      "Validation - Loss: 1.8318\n",
      "Starting Epoch 39\n",
      "Batch 0/44, Loss: 1.2855\n",
      "Epoch 39 finished\n",
      "Training - Loss: 1.2949\n",
      "Starting Epoch 40\n",
      "Batch 0/44, Loss: 1.2837\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.2941\n",
      "Epoch 40 finished\n",
      "average training loss is 1.2941\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.2941\n",
      "Validation - Loss: 1.8295\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.25)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,\n",
    "    batch_size=1024,  # Changed from 1024\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "model = ResNeXtCIFAR(cardinality=8, width=64, num_classes=100).to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-3,  # Changed from 1e-3\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-2,  # Changed from 3e-3\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1000.0\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Changed from 1.0\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a588d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 70.25%\n",
      "Standard Test Accuracy: 70.2500 (7025.00%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(model)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
