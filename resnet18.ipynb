{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404bdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('bn1_1', nn.BatchNorm2d(num_filters)),\n",
    "            ('relu1_1', nn.ReLU()),\n",
    "            ('conv1_1', nn.Conv2d(in_channels, num_filters, kernel_size=(3, 3), stride=stride, padding=1)),\n",
    "            ('bn1_2', nn.BatchNorm2d(num_filters))\n",
    "            ('conv1_2',  nn.Conv2d(num_filters, num_filters, kernel_size=(3, 3), padding=1)),\n",
    "        ]))\n",
    "        self.last_relu = nn.ReLU()\n",
    "        if stride != 1 or in_channels != num_filters:\n",
    "            self.shortcut = nn.Sequential(OrderedDict([\n",
    "                ('sho_conv_1', nn.Conv2d(in_channels, num_filters, kernel_size=(1, 1), stride=stride)),\n",
    "                ('sho_bn1', nn.BatchNorm2d(num_filters))\n",
    "            ]))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x) + self.shortcut(x)\n",
    "        x = self.last_relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1_1', nn.Conv2d(3, 64, kernel_size=(3, 3), stride=2)),\n",
    "            ('bn1_1', nn.BatchNorm2d(64)),\n",
    "            ('relu1_1', nn.ReLU()),\n",
    "            ('block1', Block(in_channels=64, num_filters=64)),\n",
    "            ('block2', Block(in_channels=64, num_filters=64)),\n",
    "            ('block3', Block(in_channels=64, num_filters=128, stride=2)),\n",
    "            ('block4', Block(in_channels=128, num_filters=128)),\n",
    "            ('block5', Block(in_channels=128, num_filters=256, stride=2)),\n",
    "            ('block6', Block(in_channels=256, num_filters=256)),\n",
    "            ('block7', Block(in_channels=256, num_filters=512, stride=2)),\n",
    "            ('block8', Block(in_channels=512, num_filters=512)),\n",
    "        ]))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a00cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n",
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.2126\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "Training - Loss: 3.8563\n",
      "Epoch 2 finished\n",
      "average training loss is 3.8563\n",
      "Epoch 2 finished\n",
      "Training - Loss: 3.8563\n",
      "Validation - Loss: 3.6542\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "Training - Loss: 3.6387\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.4621\n",
      "Epoch 4 finished\n",
      "average training loss is 3.4621\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.4621\n",
      "Validation - Loss: 3.2964\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.2996\n",
      "Starting Epoch 6\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.1720\n",
      "Epoch 6 finished\n",
      "average training loss is 3.1720\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.1720\n",
      "Validation - Loss: 3.3164\n",
      "Starting Epoch 7\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.0592\n",
      "Starting Epoch 8\n",
      "Epoch 8 finished\n",
      "Training - Loss: 2.9488\n",
      "Epoch 8 finished\n",
      "average training loss is 2.9488\n",
      "Epoch 8 finished\n",
      "Training - Loss: 2.9488\n",
      "Validation - Loss: 2.9748\n",
      "Starting Epoch 9\n",
      "Epoch 9 finished\n",
      "Training - Loss: 2.8605\n",
      "Starting Epoch 10\n",
      "Epoch 10 finished\n",
      "Training - Loss: 2.7732\n",
      "Epoch 10 finished\n",
      "average training loss is 2.7732\n",
      "Epoch 10 finished\n",
      "Training - Loss: 2.7732\n",
      "Validation - Loss: 2.8929\n",
      "Starting Epoch 11\n",
      "Epoch 11 finished\n",
      "Training - Loss: 2.6909\n",
      "Starting Epoch 12\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.6184\n",
      "Epoch 12 finished\n",
      "average training loss is 2.6184\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.6184\n",
      "Validation - Loss: 2.6829\n",
      "Starting Epoch 13\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.5274\n",
      "Starting Epoch 14\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.4605\n",
      "Epoch 14 finished\n",
      "average training loss is 2.4605\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.4605\n",
      "Validation - Loss: 2.4341\n",
      "Starting Epoch 15\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.3902\n",
      "Starting Epoch 16\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.3138\n",
      "Epoch 16 finished\n",
      "average training loss is 2.3138\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.3138\n",
      "Validation - Loss: 2.5000\n",
      "Starting Epoch 17\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.2377\n",
      "Starting Epoch 18\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.1910\n",
      "Epoch 18 finished\n",
      "average training loss is 2.1910\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.1910\n",
      "Validation - Loss: 2.3139\n",
      "Starting Epoch 19\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.1172\n",
      "Starting Epoch 20\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.0498\n",
      "Epoch 20 finished\n",
      "average training loss is 2.0498\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.0498\n",
      "Validation - Loss: 2.3944\n",
      "Starting Epoch 21\n",
      "Epoch 21 finished\n",
      "Training - Loss: 1.9866\n",
      "Starting Epoch 22\n",
      "Epoch 22 finished\n",
      "Training - Loss: 1.9335\n",
      "Epoch 22 finished\n",
      "average training loss is 1.9335\n",
      "Epoch 22 finished\n",
      "Training - Loss: 1.9335\n",
      "Validation - Loss: 2.1963\n",
      "Starting Epoch 23\n",
      "Epoch 23 finished\n",
      "Training - Loss: 1.8724\n",
      "Starting Epoch 24\n",
      "Epoch 24 finished\n",
      "Training - Loss: 1.8159\n",
      "Epoch 24 finished\n",
      "average training loss is 1.8159\n",
      "Epoch 24 finished\n",
      "Training - Loss: 1.8159\n",
      "Validation - Loss: 2.1595\n",
      "Starting Epoch 25\n",
      "Epoch 25 finished\n",
      "Training - Loss: 1.7528\n",
      "Starting Epoch 26\n",
      "Epoch 26 finished\n",
      "Training - Loss: 1.6916\n",
      "Epoch 26 finished\n",
      "average training loss is 1.6916\n",
      "Epoch 26 finished\n",
      "Training - Loss: 1.6916\n",
      "Validation - Loss: 2.0743\n",
      "Starting Epoch 27\n",
      "Epoch 27 finished\n",
      "Training - Loss: 1.6378\n",
      "Starting Epoch 28\n",
      "Epoch 28 finished\n",
      "Training - Loss: 1.5788\n",
      "Epoch 28 finished\n",
      "average training loss is 1.5788\n",
      "Epoch 28 finished\n",
      "Training - Loss: 1.5788\n",
      "Validation - Loss: 2.0616\n",
      "Starting Epoch 29\n",
      "Epoch 29 finished\n",
      "Training - Loss: 1.5116\n",
      "Starting Epoch 30\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.4593\n",
      "Epoch 30 finished\n",
      "average training loss is 1.4593\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.4593\n",
      "Validation - Loss: 2.0357\n",
      "Starting Epoch 31\n",
      "Epoch 31 finished\n",
      "Training - Loss: 1.4113\n",
      "Starting Epoch 32\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.3461\n",
      "Epoch 32 finished\n",
      "average training loss is 1.3461\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.3461\n",
      "Validation - Loss: 2.0289\n",
      "Starting Epoch 33\n",
      "Epoch 33 finished\n",
      "Training - Loss: 1.3108\n",
      "Starting Epoch 34\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.2643\n",
      "Epoch 34 finished\n",
      "average training loss is 1.2643\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.2643\n",
      "Validation - Loss: 2.0031\n",
      "Starting Epoch 35\n",
      "Epoch 35 finished\n",
      "Training - Loss: 1.2306\n",
      "Starting Epoch 36\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.2052\n",
      "Epoch 36 finished\n",
      "average training loss is 1.2052\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.2052\n",
      "Validation - Loss: 1.9921\n",
      "Starting Epoch 37\n",
      "Epoch 37 finished\n",
      "Training - Loss: 1.1837\n",
      "Starting Epoch 38\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.1735\n",
      "Epoch 38 finished\n",
      "average training loss is 1.1735\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.1735\n",
      "Validation - Loss: 1.9894\n",
      "Starting Epoch 39\n",
      "Epoch 39 finished\n",
      "Training - Loss: 1.1692\n",
      "Starting Epoch 40\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.1656\n",
      "Epoch 40 finished\n",
      "average training loss is 1.1656\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.1656\n",
      "Validation - Loss: 1.9891\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.5)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "resnet = ResNet18().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    resnet.parameters(),\n",
    "    lr=3e-3,  # Keep this for now, let OneCycleLR handle it\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-3,                # Slightly lower peak LR for stability\n",
    "    epochs=num_epochs,          # Keep 45 epochs\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.4,              # Increase warmup to 40% (18 epochs)\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=12.0,            # Start LR = 5e-3 / 12 = 4.2e-4\n",
    "    final_div_factor=400.0      # Final LR = 5e-3 / 400 = 1.25e-5\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    resnet.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = resnet(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        resnet.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = resnet(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065da948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 65.22%\n",
      "Standard Test Accuracy: 65.2200 (6522.00%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(resnet)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
