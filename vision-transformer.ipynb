{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc810805",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics mlflow databricks-cli requests optuna joblib optuna-integration[mlflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f76e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "import os\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials from environment variables\n",
    "DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\")\n",
    "DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# Set environment variables for MLflow\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "\n",
    "# Configure MLflow to use Databricks\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    # Set or create experiment\n",
    "    experiment_name = \"/Users/colizu2020@gmail.com/cifar-100-vision-transformer\"  # Replace with your email\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(\"‚úÖ Successfully connected to Databricks MLflow!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, experiment_name: str = \"/Users/colizu2020@gmail.com/cifar100\"):\n",
    "        # Set Databricks as tracking URI\n",
    "        mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "        # Set or create experiment\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    def start_run(self, run_name: str = None):\n",
    "        \"\"\"Start a new MLflow run\"\"\"\n",
    "        # Ensure any active run is ended before starting a new one,\n",
    "        # which can happen in interactive environments if a previous run was interrupted.\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run()\n",
    "\n",
    "        if run_name is None:\n",
    "            run_name = f\"cifar100_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "        return mlflow.start_run(run_name=run_name)\n",
    "\n",
    "    def log_params(self, params: dict):\n",
    "        \"\"\"Log hyperparameters\"\"\"\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "    def log_metrics(self, metrics: dict, step: int = None):\n",
    "        \"\"\"Log training metrics\"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            if step is not None:\n",
    "                mlflow.log_metric(key, value, step=step)\n",
    "            else:\n",
    "                mlflow.log_metric(key, value)\n",
    "\n",
    "    def log_model(self, model, artifact_path: str = \"model\", registered_model_name: str = None):\n",
    "        \"\"\"\n",
    "        Log PyTorch model with proper parameter separation\n",
    "\n",
    "        Args:\n",
    "            model: The trained PyTorch model\n",
    "            artifact_path: Where to store model files in the run (simple path)\n",
    "            registered_model_name: Name for Unity Catalog registration (catalog.schema.model_name)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if registered_model_name:\n",
    "                # Register model in Unity Catalog\n",
    "                mlflow.pytorch.log_model(\n",
    "                    pytorch_model=model,\n",
    "                    artifact_path=artifact_path,  # Simple path like \"model\"\n",
    "                    registered_model_name=registered_model_name  # Unity Catalog name\n",
    "                )\n",
    "                print(f\"‚úÖ Model logged and registered as: {registered_model_name}\")\n",
    "            else:\n",
    "                # Just log as artifact without registration\n",
    "                mlflow.pytorch.log_model(\n",
    "                    pytorch_model=model,\n",
    "                    artifact_path=artifact_path\n",
    "                )\n",
    "                print(f\"‚úÖ Model logged as artifact at: {artifact_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model logging failed: {e}\")\n",
    "            # Fallback: basic artifact logging\n",
    "            try:\n",
    "                mlflow.pytorch.log_model(pytorch_model=model, artifact_path=artifact_path)\n",
    "                print(f\"‚úÖ Fallback: Model logged as artifact only\")\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"‚ùå All model logging failed: {fallback_error}\")\n",
    "\n",
    "    def log_artifact(self, local_path: str, artifact_path: str = None):\n",
    "        \"\"\"Log files/artifacts\"\"\"\n",
    "        mlflow.log_artifact(local_path, artifact_path)\n",
    "\n",
    "    def log_figure(self, figure, filename: str):\n",
    "        \"\"\"Log matplotlib figures\"\"\"\n",
    "        import tempfile\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "            figure.savefig(tmp.name, dpi=150, bbox_inches='tight')\n",
    "            mlflow.log_artifact(tmp.name, f\"plots/{filename}\")\n",
    "\n",
    "        plt.close(figure)  # Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "config = {\n",
    "    \"patch_size\": 4,\n",
    "    \"num_classes\": 100,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_hidden_layers\": 8,\n",
    "    \"hidden_size\": 384,\n",
    "    \"image_size\": 32,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"num_attent_heads\": 6,\n",
    "    \"intermediate_size\": 4 * 384,\n",
    "    \"qkv_bias\": True,\n",
    "    \"initializer_range\": 0.02\n",
    "}\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_channels, height, width)\n",
    "        new_x: (batch_size, num_patches, hidden_size)\n",
    "        \"\"\"\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config[\"hidden_size\"])) # Corrected size\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings # Used position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        self.dropout = nn.Dropout(dropout) # Keep this to use the passed dropout rate\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Apply dropout to attention scores before softmax\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attention_scores = self.dropout(attention_probs)\n",
    "\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attent_heads = config[\"num_attent_heads\"]\n",
    "\n",
    "        self.attent_head_size = self.hidden_size // self.num_attent_heads # Fixed typo\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        self.heads = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(self.num_attent_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attent_head_size,\n",
    "                config[\"dropout_rate\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "\n",
    "            self.heads.append(head)\n",
    "\n",
    "        self.all_head_size = self.num_attent_heads * self.attent_head_size # Added initialization for all_head_size\n",
    "\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat([attent for attent, _ in attention_outputs], dim=-1)\n",
    "\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.dropout_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1 - self.dropout_prob\n",
    "        shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.attention = MultiHeadedAttention(config)\n",
    "        self.drop_path1 = DropPath(drop_path_rate)\n",
    "        self.drop_path2 = DropPath(drop_path_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attention_input = self.layer_norm1(x)\n",
    "        attention_output = self.attention(attention_input) # Removed attention_probs as it's not returned by MultiHeadedAttention\n",
    "        attention_output = self.drop_path1(attention_output)\n",
    "        x = x + attention_output # Add skip connection for attention output\n",
    "\n",
    "        mlp_input = self.layer_norm2(x)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        mlp_output = self.drop_path2(mlp_output)\n",
    "        x = x + mlp_output # Add skip connection for MLP output\n",
    "\n",
    "        return x, attention_output # Returning attention_output for consistency if needed later, but only x is used        \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "\n",
    "    def forward(self, x, output_attentions):\n",
    "        all_attention = []\n",
    "\n",
    "        for block in self.blocks:\n",
    "            # The block forward method has been updated to return `x, attention_output`\n",
    "            # We can capture attention_output if output_attentions is True.\n",
    "            x, attention_output_for_block = block(x)\n",
    "            if output_attentions:\n",
    "                all_attention.append(attention_output_for_block) # Storing attention_output, not attention_probs\n",
    "\n",
    "        return (x, all_attention)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "        self.embedding = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        embedding_output = self.embedding(x)\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions)\n",
    "        logits = self.classifier(encoder_output[:, 0, :])\n",
    "\n",
    "        if output_attentions:\n",
    "           return (logits, all_attentions)\n",
    "        return logits # Return logits if output_attentions is False\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Hyperparameter Search - Colab-Friendly Version\n",
    "Run ONE trial at a time, resume later\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torchmetrics\n",
    "from torch.amp import GradScaler, autocast\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import mlflow\n",
    "import math\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import gc \n",
    "\n",
    "def mixup_cutmix(inputs, targets, alpha=0.8, cutmix_prob=0.5):\n",
    "    \"\"\"Apply MixUp or CutMix randomly\"\"\"\n",
    "    batch_size = inputs.size(0)\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    \n",
    "    rand_index = torch.randperm(batch_size).to(inputs.device)\n",
    "    \n",
    "    if np.random.rand() < cutmix_prob:\n",
    "        # CutMix\n",
    "        _, _, H, W = inputs.shape\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
    "        cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "        \n",
    "        x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        inputs[:, :, y1:y2, x1:x2] = inputs[rand_index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "    else:\n",
    "        # MixUp\n",
    "        inputs = lam * inputs + (1 - lam) * inputs[rand_index]\n",
    "    \n",
    "    return inputs, targets, targets[rand_index], lam\n",
    "\n",
    "def get_data_loaders():\n",
    "    \"\"\"Optimized data loaders\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.TrivialAugmentWide(),  # Faster than AutoAugment\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "        transforms.RandomErasing(p=0.25)\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "    ])\n",
    "\n",
    "    cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "    train_size = int(0.9 * len(cifar_train_raw))\n",
    "    \n",
    "    cifar_train = Subset(\n",
    "        datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=train_transform), \n",
    "        list(range(train_size))\n",
    "    )\n",
    "    cifar_val = Subset(\n",
    "        datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform), \n",
    "        list(range(train_size, len(cifar_train_raw)))\n",
    "    )\n",
    "\n",
    "    # Larger batch size + fewer workers on Colab\n",
    "    train_loader = DataLoader(\n",
    "        cifar_train, \n",
    "        batch_size=1024,  # Increase from 256\n",
    "        shuffle=True, \n",
    "        num_workers=2, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        drop_last=True,  # Faster - avoids small final batch\n",
    "        prefetch_factor=6\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        cifar_val, \n",
    "        batch_size=512,  # Increase from 512\n",
    "        shuffle=False, \n",
    "        num_workers=2, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=6\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_single_config(hp_config, num_epochs=30):\n",
    "    \"\"\"Train model with a specific config - single trial\"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create model config\n",
    "    model_config = {\n",
    "        \"patch_size\": 4,\n",
    "        \"num_classes\": 100,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_hidden_layers\": 4,    # Reduced from 6\n",
    "        \"hidden_size\": 192,        # Reduced from 256\n",
    "        \"image_size\": 32,\n",
    "        \"dropout_rate\": hp_config[\"dropout_rate\"],\n",
    "        \"num_attent_heads\": 6,     # Reduced from 8\n",
    "        \"intermediate_size\": 768,  # Reduced from 1024\n",
    "        \"qkv_bias\": True,\n",
    "        \"initializer_range\": 0.02,\n",
    "    }\n",
    "\n",
    "    model = ViT(model_config).to(device)\n",
    "    print(f\"‚úÖ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "    if hasattr(torch, 'compile'):\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "        print(\"‚úÖ Model compiled with torch.compile\")\n",
    "\n",
    "    print(\"üì¶ Loading data...\")\n",
    "    train_loader, val_loader = get_data_loaders()\n",
    "    print(f\"‚úÖ Data loaded: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n",
    "\n",
    "    # Training setup\n",
    "    loss_function = nn.CrossEntropyLoss(label_smoothing=hp_config[\"label_smoothing\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hp_config[\"learning_rate\"],\n",
    "        weight_decay=hp_config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Warmup + Cosine schedule\n",
    "    warmup_epochs = hp_config[\"warmup_epochs\"]\n",
    "    def get_lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)\n",
    "            return max(1e-6 / hp_config[\"learning_rate\"], 0.5 * (1 + math.cos(math.pi * progress)))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=100).to(device)\n",
    "    val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=100).to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_accuracy.reset()\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if epoch < num_epochs - 10:\n",
    "                inputs, targets_a, targets_b , lam= mixup_cutmix(inputs, targets)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = lam * loss_function(outputs, targets_a) + \\\n",
    "                           (1 - lam) * loss_function(outputs, targets_b)\n",
    "            else:\n",
    "                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_function(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy.update(outputs.detach(), targets)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_accuracy.compute().item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:\n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_accuracy.reset()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_function(outputs, targets)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_accuracy.update(outputs, targets)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_accuracy.compute().item()\n",
    "\n",
    "            # Track best\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_loss = avg_val_loss\n",
    "\n",
    "            # Log to MLflow\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            }, step=epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Best: {best_val_acc:.4f}\")\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"final_train_accuracy\": train_acc,\n",
    "        \"final_val_accuracy\": val_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Pre-defined configs to try one at a time\n",
    "PRIORITY_CONFIGS = [\n",
    "    # Config 1: Balanced (Best starting point)\n",
    "    {\n",
    "        \"name\": \"improved_vit\",\n",
    "        \"learning_rate\": 1e-3,        # Lower LR for larger model\n",
    "        \"weight_decay\": 0.05,         # Less aggressive\n",
    "        \"warmup_epochs\": 5,          # Longer warmup\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"label_smoothing\": 0.1\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def run_single_trial(trial_index=0, num_epochs=30, experiment_name=\"/Users/colizu2020@gmail.com/cifar-100-vit-manual\"):\n",
    "    \"\"\"\n",
    "    Run a SINGLE trial - perfect for Colab free tier\n",
    "\n",
    "    Args:\n",
    "        trial_index: Which config to run (0-7)\n",
    "        num_epochs: How many epochs (30 for quick test, 50 for final)\n",
    "        experiment_name: MLflow experiment name\n",
    "    \"\"\"\n",
    "\n",
    "    if trial_index >= len(PRIORITY_CONFIGS):\n",
    "        print(f\"‚ùå Invalid trial_index {trial_index}. Must be 0-{len(PRIORITY_CONFIGS)-1}\")\n",
    "        return\n",
    "\n",
    "    hp_config = PRIORITY_CONFIGS[trial_index]\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"RUNNING TRIAL {trial_index + 1}/{len(PRIORITY_CONFIGS)}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Config: {hp_config['name']}\")\n",
    "    print(f\"Hyperparameters:\")\n",
    "    for key, value in hp_config.items():\n",
    "        if key != 'name':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "\n",
    "    # Set MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"{hp_config['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({k: v for k, v in hp_config.items() if k != 'name'})\n",
    "        mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "        mlflow.log_param(\"trial_index\", trial_index)\n",
    "\n",
    "        # Train model\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        metrics = train_single_config(hp_config, num_epochs=num_epochs)\n",
    "\n",
    "        # Log final metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"best_val_accuracy\": metrics[\"best_val_accuracy\"],\n",
    "            \"best_val_loss\": metrics[\"best_val_loss\"],\n",
    "            \"final_train_accuracy\": metrics[\"final_train_accuracy\"],\n",
    "            \"final_val_accuracy\": metrics[\"final_val_accuracy\"],\n",
    "        })\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRIAL COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Best Val Accuracy: {metrics['best_val_accuracy']:.4f}\")\n",
    "    print(f\"‚úÖ Best Val Loss: {metrics['best_val_loss']:.4f}\")\n",
    "    print(f\"‚úÖ MLflow Run ID: {run_id}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Save progress\n",
    "    progress_file = \"search_progress.txt\"\n",
    "    with open(progress_file, \"a\") as f:\n",
    "        f.write(f\"\\nTrial {trial_index}: {hp_config['name']}\\n\")\n",
    "        f.write(f\"Val Acc: {metrics['best_val_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Run ID: {run_id}\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "\n",
    "    print(f\"\\nüìù Progress saved to {progress_file}\")\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(f\"   - Run trial {trial_index + 1} next: run_single_trial(trial_index={trial_index + 1})\")\n",
    "    print(f\"   - Check MLflow UI to compare results\")\n",
    "    print(f\"   - Total trials remaining: {len(PRIORITY_CONFIGS) - trial_index - 1}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def show_all_configs():\n",
    "    \"\"\"Display all available configs\"\"\"\n",
    "    print(\"\\nAVAILABLE CONFIGURATIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    for i, config in enumerate(PRIORITY_CONFIGS):\n",
    "        print(f\"\\nTrial {i}: {config['name']}\")\n",
    "        print(\"-\"*40)\n",
    "        for key, value in config.items():\n",
    "            if key != 'name':\n",
    "                print(f\"  {key:20s}: {value}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nTotal configs: {len(PRIORITY_CONFIGS)}\")\n",
    "    print(\"\\nTo run a specific config:\")\n",
    "    print(\"  run_single_trial(trial_index=0)  # Run first config\")\n",
    "    print(\"  run_single_trial(trial_index=1)  # Run second config\")\n",
    "    print(\"  etc...\")\n",
    "\n",
    "\n",
    "# Run single trial\n",
    "run_single_trial(trial_index=0, num_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
