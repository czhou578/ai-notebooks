{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84943973",
   "metadata": {},
   "source": [
    "Paper link: https://arxiv.org/pdf/1409.1556\n",
    "\n",
    "# Architecture:\n",
    "\n",
    "The convolution stride is fixed to 1 pixel;\n",
    "All hidden layers are equipped with the rectification (RELU)\n",
    "the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by\n",
    "five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed\n",
    "by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.\n",
    "\n",
    "A stack of convolutional layers (which has a different depth in different architectures) is followed by\n",
    "three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-\n",
    "way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is\n",
    "the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n",
    "\n",
    "Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with\n",
    "very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.\n",
    "\n",
    "The original VGG paper (2014) managed this by pre-training shallow networks (Configuration A) and using those weights to initialize the deeper ones (like Configuration B). They didn't just train the deep networks from scratch with random initialization because gradients would vanish or explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345a675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# filepath: [vgg.ipynb](http://_vscodecontentref_/0)\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512), # <--- ADDED\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),            \n",
    "            nn.Linear(512, 100)            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22d946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "100%|██████████| 169M/169M [00:05<00:00, 30.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 15,299,748\n",
      "Starting Epoch 1\n",
      "Batch 0/352, Loss: 4.6042\n",
      "Batch 50/352, Loss: 4.5856\n",
      "Batch 100/352, Loss: 4.5243\n",
      "Batch 150/352, Loss: 4.5233\n",
      "Batch 200/352, Loss: 4.5052\n",
      "Batch 250/352, Loss: 4.4837\n",
      "Batch 300/352, Loss: 4.4433\n",
      "Batch 350/352, Loss: 4.4099\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.5030\n",
      "Starting Epoch 2\n",
      "Batch 0/352, Loss: 4.3311\n",
      "Batch 50/352, Loss: 4.3199\n",
      "Batch 100/352, Loss: 4.4114\n",
      "Batch 150/352, Loss: 4.4540\n",
      "Batch 200/352, Loss: 4.2587\n",
      "Batch 250/352, Loss: 4.2790\n",
      "Batch 300/352, Loss: 4.1727\n",
      "Batch 350/352, Loss: 4.4007\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.3190\n",
      "Epoch 2 finished\n",
      "average training loss is 4.3190\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.3190\n",
      "Validation - Loss: 4.1683\n",
      "Starting Epoch 3\n",
      "Batch 0/352, Loss: 4.1792\n",
      "Batch 50/352, Loss: 4.1509\n",
      "Batch 100/352, Loss: 4.2967\n",
      "Batch 150/352, Loss: 4.1913\n",
      "Batch 200/352, Loss: 4.1644\n",
      "Batch 250/352, Loss: 4.1844\n",
      "Batch 300/352, Loss: 4.0072\n",
      "Batch 350/352, Loss: 4.0200\n",
      "Epoch 3 finished\n",
      "Training - Loss: 4.1567\n",
      "Starting Epoch 4\n",
      "Batch 0/352, Loss: 4.1213\n",
      "Batch 50/352, Loss: 4.0312\n",
      "Batch 100/352, Loss: 3.9805\n",
      "Batch 150/352, Loss: 4.0026\n",
      "Batch 200/352, Loss: 4.0020\n",
      "Batch 250/352, Loss: 4.0176\n",
      "Batch 300/352, Loss: 3.8910\n",
      "Batch 350/352, Loss: 3.7477\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.9687\n",
      "Epoch 4 finished\n",
      "average training loss is 3.9687\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.9687\n",
      "Validation - Loss: 3.8846\n",
      "Starting Epoch 5\n",
      "Batch 0/352, Loss: 3.9003\n",
      "Batch 50/352, Loss: 3.9032\n",
      "Batch 100/352, Loss: 3.7805\n",
      "Batch 150/352, Loss: 3.6923\n",
      "Batch 200/352, Loss: 3.6430\n",
      "Batch 250/352, Loss: 3.6936\n",
      "Batch 300/352, Loss: 3.7308\n",
      "Batch 350/352, Loss: 3.7624\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.7981\n",
      "Starting Epoch 6\n",
      "Batch 0/352, Loss: 3.5684\n",
      "Batch 50/352, Loss: 3.6965\n",
      "Batch 100/352, Loss: 3.6777\n",
      "Batch 150/352, Loss: 3.7401\n",
      "Batch 200/352, Loss: 3.4945\n",
      "Batch 250/352, Loss: 3.6638\n",
      "Batch 300/352, Loss: 3.6999\n",
      "Batch 350/352, Loss: 3.4623\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.6572\n",
      "Epoch 6 finished\n",
      "average training loss is 3.6572\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.6572\n",
      "Validation - Loss: 3.5210\n",
      "Starting Epoch 7\n",
      "Batch 0/352, Loss: 3.5114\n",
      "Batch 50/352, Loss: 3.4086\n",
      "Batch 100/352, Loss: 3.7285\n",
      "Batch 150/352, Loss: 3.4336\n",
      "Batch 200/352, Loss: 3.4820\n",
      "Batch 250/352, Loss: 3.4854\n",
      "Batch 300/352, Loss: 3.3848\n",
      "Batch 350/352, Loss: 3.5774\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.4994\n",
      "Starting Epoch 8\n",
      "Batch 0/352, Loss: 3.3356\n",
      "Batch 50/352, Loss: 3.5046\n",
      "Batch 100/352, Loss: 3.6304\n",
      "Batch 150/352, Loss: 3.4039\n",
      "Batch 200/352, Loss: 3.2948\n",
      "Batch 250/352, Loss: 3.3422\n",
      "Batch 300/352, Loss: 3.2693\n",
      "Batch 350/352, Loss: 3.4187\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.3761\n",
      "Epoch 8 finished\n",
      "average training loss is 3.3761\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.3761\n",
      "Validation - Loss: 3.3493\n",
      "Starting Epoch 9\n",
      "Batch 0/352, Loss: 3.2471\n",
      "Batch 50/352, Loss: 3.3831\n",
      "Batch 100/352, Loss: 3.4344\n",
      "Batch 150/352, Loss: 3.3996\n",
      "Batch 200/352, Loss: 3.2866\n",
      "Batch 250/352, Loss: 3.3491\n",
      "Batch 300/352, Loss: 3.2479\n",
      "Batch 350/352, Loss: 3.1825\n",
      "Epoch 9 finished\n",
      "Training - Loss: 3.2552\n",
      "Starting Epoch 10\n",
      "Batch 0/352, Loss: 3.1676\n",
      "Batch 50/352, Loss: 3.1731\n",
      "Batch 100/352, Loss: 3.3286\n",
      "Batch 150/352, Loss: 3.1589\n",
      "Batch 200/352, Loss: 3.0429\n",
      "Batch 250/352, Loss: 3.1406\n",
      "Batch 300/352, Loss: 3.2609\n",
      "Batch 350/352, Loss: 3.4107\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.1625\n",
      "Epoch 10 finished\n",
      "average training loss is 3.1625\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.1625\n",
      "Validation - Loss: 3.3538\n",
      "Starting Epoch 11\n",
      "Batch 0/352, Loss: 3.1434\n",
      "Batch 50/352, Loss: 3.0405\n",
      "Batch 100/352, Loss: 3.0941\n",
      "Batch 150/352, Loss: 3.0216\n",
      "Batch 200/352, Loss: 3.0768\n",
      "Batch 250/352, Loss: 3.1675\n",
      "Batch 300/352, Loss: 2.9965\n",
      "Batch 350/352, Loss: 3.2239\n",
      "Epoch 11 finished\n",
      "Training - Loss: 3.0785\n",
      "Starting Epoch 12\n",
      "Batch 0/352, Loss: 3.0614\n",
      "Batch 50/352, Loss: 2.8011\n",
      "Batch 100/352, Loss: 2.9000\n",
      "Batch 150/352, Loss: 3.1639\n",
      "Batch 200/352, Loss: 2.8865\n",
      "Batch 250/352, Loss: 3.1417\n",
      "Batch 300/352, Loss: 2.9185\n",
      "Batch 350/352, Loss: 3.0447\n",
      "Epoch 12 finished\n",
      "Training - Loss: 3.0041\n",
      "Epoch 12 finished\n",
      "average training loss is 3.0041\n",
      "Epoch 12 finished\n",
      "Training - Loss: 3.0041\n",
      "Validation - Loss: 2.8623\n",
      "Starting Epoch 13\n",
      "Batch 0/352, Loss: 3.0838\n",
      "Batch 50/352, Loss: 2.9721\n",
      "Batch 100/352, Loss: 2.8958\n",
      "Batch 150/352, Loss: 2.9870\n",
      "Batch 200/352, Loss: 3.0930\n",
      "Batch 250/352, Loss: 3.0019\n",
      "Batch 300/352, Loss: 2.9212\n",
      "Batch 350/352, Loss: 2.9330\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.9339\n",
      "Starting Epoch 14\n",
      "Batch 0/352, Loss: 2.9708\n",
      "Batch 50/352, Loss: 3.0088\n",
      "Batch 100/352, Loss: 2.6985\n",
      "Batch 150/352, Loss: 2.8833\n",
      "Batch 200/352, Loss: 3.0965\n",
      "Batch 250/352, Loss: 3.1237\n",
      "Batch 300/352, Loss: 2.6439\n",
      "Batch 350/352, Loss: 2.7632\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.8777\n",
      "Epoch 14 finished\n",
      "average training loss is 2.8777\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.8777\n",
      "Validation - Loss: 2.8291\n",
      "Starting Epoch 15\n",
      "Batch 0/352, Loss: 2.9204\n",
      "Batch 50/352, Loss: 2.7028\n",
      "Batch 100/352, Loss: 2.8865\n",
      "Batch 150/352, Loss: 2.7368\n",
      "Batch 200/352, Loss: 2.9330\n",
      "Batch 250/352, Loss: 2.6886\n",
      "Batch 300/352, Loss: 2.7909\n",
      "Batch 350/352, Loss: 2.8924\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.8276\n",
      "Starting Epoch 16\n",
      "Batch 0/352, Loss: 2.7623\n",
      "Batch 50/352, Loss: 2.6226\n",
      "Batch 100/352, Loss: 2.8285\n",
      "Batch 150/352, Loss: 2.6837\n",
      "Batch 200/352, Loss: 2.9150\n",
      "Batch 250/352, Loss: 2.7259\n",
      "Batch 300/352, Loss: 2.7225\n",
      "Batch 350/352, Loss: 2.7432\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.7781\n",
      "Epoch 16 finished\n",
      "average training loss is 2.7781\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.7781\n",
      "Validation - Loss: 2.7600\n",
      "Starting Epoch 17\n",
      "Batch 0/352, Loss: 2.8139\n",
      "Batch 50/352, Loss: 2.7184\n",
      "Batch 100/352, Loss: 2.7702\n",
      "Batch 150/352, Loss: 2.7311\n",
      "Batch 200/352, Loss: 2.8688\n",
      "Batch 250/352, Loss: 2.7516\n",
      "Batch 300/352, Loss: 2.8332\n",
      "Batch 350/352, Loss: 2.8274\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.7431\n",
      "Starting Epoch 18\n",
      "Batch 0/352, Loss: 2.6178\n",
      "Batch 50/352, Loss: 2.7424\n",
      "Batch 100/352, Loss: 2.7520\n",
      "Batch 150/352, Loss: 2.6745\n",
      "Batch 200/352, Loss: 2.7936\n",
      "Batch 250/352, Loss: 2.6904\n",
      "Batch 300/352, Loss: 2.6792\n",
      "Batch 350/352, Loss: 2.6593\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.6970\n",
      "Epoch 18 finished\n",
      "average training loss is 2.6970\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.6970\n",
      "Validation - Loss: 2.6031\n",
      "Starting Epoch 19\n",
      "Batch 0/352, Loss: 2.7694\n",
      "Batch 50/352, Loss: 2.5319\n",
      "Batch 100/352, Loss: 2.4748\n",
      "Batch 150/352, Loss: 2.7440\n",
      "Batch 200/352, Loss: 2.8490\n",
      "Batch 250/352, Loss: 2.4614\n",
      "Batch 300/352, Loss: 2.8215\n",
      "Batch 350/352, Loss: 2.4792\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.6704\n",
      "Starting Epoch 20\n",
      "Batch 0/352, Loss: 2.5027\n",
      "Batch 50/352, Loss: 2.7707\n",
      "Batch 100/352, Loss: 2.6096\n",
      "Batch 150/352, Loss: 2.6944\n",
      "Batch 200/352, Loss: 2.5072\n",
      "Batch 250/352, Loss: 2.6091\n",
      "Batch 300/352, Loss: 2.5545\n",
      "Batch 350/352, Loss: 2.8167\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.6413\n",
      "Epoch 20 finished\n",
      "average training loss is 2.6413\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.6413\n",
      "Validation - Loss: 2.5688\n",
      "Starting Epoch 21\n",
      "Batch 0/352, Loss: 2.5385\n",
      "Batch 50/352, Loss: 2.7856\n",
      "Batch 100/352, Loss: 2.5075\n",
      "Batch 150/352, Loss: 2.3606\n",
      "Batch 200/352, Loss: 2.7345\n",
      "Batch 250/352, Loss: 2.6451\n",
      "Batch 300/352, Loss: 2.7370\n",
      "Batch 350/352, Loss: 2.3806\n",
      "Epoch 21 finished\n",
      "Training - Loss: 2.6038\n",
      "Starting Epoch 22\n",
      "Batch 0/352, Loss: 2.7989\n",
      "Batch 50/352, Loss: 2.3988\n",
      "Batch 100/352, Loss: 2.3916\n",
      "Batch 150/352, Loss: 2.4326\n",
      "Batch 200/352, Loss: 2.7438\n",
      "Batch 250/352, Loss: 2.5681\n",
      "Batch 300/352, Loss: 2.5513\n",
      "Batch 350/352, Loss: 2.6387\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.5767\n",
      "Epoch 22 finished\n",
      "average training loss is 2.5767\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.5767\n",
      "Validation - Loss: 2.7468\n",
      "Starting Epoch 23\n",
      "Batch 0/352, Loss: 2.4448\n",
      "Batch 50/352, Loss: 2.6424\n",
      "Batch 100/352, Loss: 2.5191\n",
      "Batch 150/352, Loss: 2.3701\n",
      "Batch 200/352, Loss: 2.4575\n",
      "Batch 250/352, Loss: 2.5970\n",
      "Batch 300/352, Loss: 2.3503\n",
      "Batch 350/352, Loss: 2.4223\n",
      "Epoch 23 finished\n",
      "Training - Loss: 2.5446\n",
      "Starting Epoch 24\n",
      "Batch 0/352, Loss: 2.2568\n",
      "Batch 50/352, Loss: 2.2342\n",
      "Batch 100/352, Loss: 2.4594\n",
      "Batch 150/352, Loss: 2.4502\n",
      "Batch 200/352, Loss: 2.5385\n",
      "Batch 250/352, Loss: 2.4250\n",
      "Batch 300/352, Loss: 2.4445\n",
      "Batch 350/352, Loss: 2.3810\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.5229\n",
      "Epoch 24 finished\n",
      "average training loss is 2.5229\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.5229\n",
      "Validation - Loss: 2.4077\n",
      "Starting Epoch 25\n",
      "Batch 0/352, Loss: 2.6604\n",
      "Batch 50/352, Loss: 2.4121\n",
      "Batch 100/352, Loss: 2.4858\n",
      "Batch 150/352, Loss: 2.5920\n",
      "Batch 200/352, Loss: 2.5479\n",
      "Batch 250/352, Loss: 2.7282\n",
      "Batch 300/352, Loss: 2.3609\n",
      "Batch 350/352, Loss: 2.4240\n",
      "Epoch 25 finished\n",
      "Training - Loss: 2.5137\n",
      "Starting Epoch 26\n",
      "Batch 0/352, Loss: 2.5387\n",
      "Batch 50/352, Loss: 2.2096\n",
      "Batch 100/352, Loss: 2.5596\n",
      "Batch 150/352, Loss: 2.6623\n",
      "Batch 200/352, Loss: 2.5127\n",
      "Batch 250/352, Loss: 2.5477\n",
      "Batch 300/352, Loss: 2.5553\n",
      "Batch 350/352, Loss: 2.4249\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.4836\n",
      "Epoch 26 finished\n",
      "average training loss is 2.4836\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.4836\n",
      "Validation - Loss: 2.4966\n",
      "Starting Epoch 27\n",
      "Batch 0/352, Loss: 2.3748\n",
      "Batch 50/352, Loss: 2.5885\n",
      "Batch 100/352, Loss: 2.6702\n",
      "Batch 150/352, Loss: 2.4077\n",
      "Batch 200/352, Loss: 2.4181\n",
      "Batch 250/352, Loss: 2.3088\n",
      "Batch 300/352, Loss: 2.6587\n",
      "Batch 350/352, Loss: 2.5308\n",
      "Epoch 27 finished\n",
      "Training - Loss: 2.4667\n",
      "Starting Epoch 28\n",
      "Batch 0/352, Loss: 2.3756\n",
      "Batch 50/352, Loss: 2.4042\n",
      "Batch 100/352, Loss: 2.4796\n",
      "Batch 150/352, Loss: 2.1992\n",
      "Batch 200/352, Loss: 2.5296\n",
      "Batch 250/352, Loss: 2.4726\n",
      "Batch 300/352, Loss: 2.7842\n",
      "Batch 350/352, Loss: 2.2354\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.4409\n",
      "Epoch 28 finished\n",
      "average training loss is 2.4409\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.4409\n",
      "Validation - Loss: 2.3495\n",
      "Starting Epoch 29\n",
      "Batch 0/352, Loss: 2.4258\n",
      "Batch 50/352, Loss: 2.3340\n",
      "Batch 100/352, Loss: 2.3605\n",
      "Batch 150/352, Loss: 2.4825\n",
      "Batch 200/352, Loss: 2.5580\n",
      "Batch 250/352, Loss: 2.4085\n",
      "Batch 300/352, Loss: 2.4792\n",
      "Batch 350/352, Loss: 2.3060\n",
      "Epoch 29 finished\n",
      "Training - Loss: 2.4304\n",
      "Starting Epoch 30\n",
      "Batch 0/352, Loss: 2.3507\n",
      "Batch 50/352, Loss: 2.4954\n",
      "Batch 100/352, Loss: 2.3236\n",
      "Batch 150/352, Loss: 2.4814\n",
      "Batch 200/352, Loss: 2.2629\n",
      "Batch 250/352, Loss: 2.4966\n",
      "Batch 300/352, Loss: 2.6375\n",
      "Batch 350/352, Loss: 2.5759\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.4246\n",
      "Epoch 30 finished\n",
      "average training loss is 2.4246\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.4246\n",
      "Validation - Loss: 2.7363\n",
      "Starting Epoch 31\n",
      "Batch 0/352, Loss: 2.4370\n",
      "Batch 50/352, Loss: 2.3977\n",
      "Batch 100/352, Loss: 2.5215\n",
      "Batch 150/352, Loss: 2.5159\n",
      "Batch 200/352, Loss: 2.3578\n",
      "Batch 250/352, Loss: 2.5931\n",
      "Batch 300/352, Loss: 2.5427\n",
      "Batch 350/352, Loss: 2.2109\n",
      "Epoch 31 finished\n",
      "Training - Loss: 2.3896\n",
      "Starting Epoch 32\n",
      "Batch 0/352, Loss: 2.7182\n",
      "Batch 50/352, Loss: 2.2676\n",
      "Batch 100/352, Loss: 2.3144\n",
      "Batch 150/352, Loss: 2.4324\n",
      "Batch 200/352, Loss: 2.4746\n",
      "Batch 250/352, Loss: 2.4966\n",
      "Batch 300/352, Loss: 2.2330\n",
      "Batch 350/352, Loss: 2.4444\n",
      "Epoch 32 finished\n",
      "Training - Loss: 2.3757\n",
      "Epoch 32 finished\n",
      "average training loss is 2.3757\n",
      "Epoch 32 finished\n",
      "Training - Loss: 2.3757\n",
      "Validation - Loss: 2.4359\n",
      "Starting Epoch 33\n",
      "Batch 0/352, Loss: 2.3474\n",
      "Batch 50/352, Loss: 2.3384\n",
      "Batch 100/352, Loss: 2.1740\n",
      "Batch 150/352, Loss: 2.5199\n",
      "Batch 200/352, Loss: 2.6310\n",
      "Batch 250/352, Loss: 2.1934\n",
      "Batch 300/352, Loss: 2.4666\n",
      "Batch 350/352, Loss: 2.6011\n",
      "Epoch 33 finished\n",
      "Training - Loss: 2.3672\n",
      "Starting Epoch 34\n",
      "Batch 0/352, Loss: 2.3306\n",
      "Batch 50/352, Loss: 2.3093\n",
      "Batch 100/352, Loss: 2.4347\n",
      "Batch 150/352, Loss: 2.5032\n",
      "Batch 200/352, Loss: 2.3900\n",
      "Batch 250/352, Loss: 2.3014\n",
      "Batch 300/352, Loss: 2.4251\n",
      "Batch 350/352, Loss: 2.5878\n",
      "Epoch 34 finished\n",
      "Training - Loss: 2.3556\n",
      "Epoch 34 finished\n",
      "average training loss is 2.3556\n",
      "Epoch 34 finished\n",
      "Training - Loss: 2.3556\n",
      "Validation - Loss: 2.3069\n",
      "Starting Epoch 35\n",
      "Batch 0/352, Loss: 2.2993\n",
      "Batch 50/352, Loss: 2.2538\n",
      "Batch 100/352, Loss: 2.1974\n",
      "Batch 150/352, Loss: 2.0479\n",
      "Batch 200/352, Loss: 2.4050\n",
      "Batch 250/352, Loss: 2.5633\n",
      "Batch 300/352, Loss: 2.5722\n",
      "Batch 350/352, Loss: 2.3660\n",
      "Epoch 35 finished\n",
      "Training - Loss: 2.3506\n",
      "Starting Epoch 36\n",
      "Batch 0/352, Loss: 2.2859\n",
      "Batch 50/352, Loss: 2.5445\n",
      "Batch 100/352, Loss: 2.4211\n",
      "Batch 150/352, Loss: 2.2777\n",
      "Batch 200/352, Loss: 2.2577\n",
      "Batch 250/352, Loss: 2.1952\n",
      "Batch 300/352, Loss: 2.4768\n",
      "Batch 350/352, Loss: 2.4827\n",
      "Epoch 36 finished\n",
      "Training - Loss: 2.3225\n",
      "Epoch 36 finished\n",
      "average training loss is 2.3225\n",
      "Epoch 36 finished\n",
      "Training - Loss: 2.3225\n",
      "Validation - Loss: 2.3126\n",
      "Starting Epoch 37\n",
      "Batch 0/352, Loss: 2.3335\n",
      "Batch 50/352, Loss: 2.3192\n",
      "Batch 100/352, Loss: 2.3509\n",
      "Batch 150/352, Loss: 2.2428\n",
      "Batch 200/352, Loss: 2.1396\n",
      "Batch 250/352, Loss: 2.3182\n",
      "Batch 300/352, Loss: 2.5049\n",
      "Batch 350/352, Loss: 2.7190\n",
      "Epoch 37 finished\n",
      "Training - Loss: 2.3240\n",
      "Starting Epoch 38\n",
      "Batch 0/352, Loss: 2.3527\n",
      "Batch 50/352, Loss: 2.3735\n",
      "Batch 100/352, Loss: 2.2836\n",
      "Batch 150/352, Loss: 2.3471\n",
      "Batch 200/352, Loss: 2.4638\n",
      "Batch 250/352, Loss: 2.3801\n",
      "Batch 300/352, Loss: 2.3983\n",
      "Batch 350/352, Loss: 2.3645\n",
      "Epoch 38 finished\n",
      "Training - Loss: 2.3069\n",
      "Epoch 38 finished\n",
      "average training loss is 2.3069\n",
      "Epoch 38 finished\n",
      "Training - Loss: 2.3069\n",
      "Validation - Loss: 2.2550\n",
      "Starting Epoch 39\n",
      "Batch 0/352, Loss: 2.3020\n",
      "Batch 50/352, Loss: 2.2264\n",
      "Batch 100/352, Loss: 2.5407\n",
      "Batch 150/352, Loss: 2.3050\n",
      "Batch 200/352, Loss: 2.4993\n",
      "Batch 250/352, Loss: 2.3923\n",
      "Batch 300/352, Loss: 2.4212\n",
      "Batch 350/352, Loss: 2.3982\n",
      "Epoch 39 finished\n",
      "Training - Loss: 2.3041\n",
      "Starting Epoch 40\n",
      "Batch 0/352, Loss: 2.4872\n",
      "Batch 50/352, Loss: 2.1782\n",
      "Batch 100/352, Loss: 2.4937\n",
      "Batch 150/352, Loss: 2.3050\n",
      "Batch 200/352, Loss: 2.0941\n",
      "Batch 250/352, Loss: 2.4663\n",
      "Batch 300/352, Loss: 2.1855\n",
      "Batch 350/352, Loss: 2.2424\n",
      "Epoch 40 finished\n",
      "Training - Loss: 2.2917\n",
      "Epoch 40 finished\n",
      "average training loss is 2.2917\n",
      "Epoch 40 finished\n",
      "Training - Loss: 2.2917\n",
      "Validation - Loss: 2.5039\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,\n",
    "    batch_size=128,   # was 1024\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,  # free worker memory\n",
    "    prefetch_factor=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = VGG().to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[60, 120, 160],  # Decay at these epochs\n",
    "    gamma=0.2  # Multiply LR by 0.2\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfc9095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 50.12%\n",
      "Standard Test Accuracy: 50.1200 (50.12%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(model)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d055636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
