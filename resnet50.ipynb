{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0b1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channel, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1_1', nn.Conv2d(in_channels, out_channel // 4, kernel_size=(1, 1))),\n",
    "            ('bn1_1', nn.BatchNorm2d(out_channel // 4)),\n",
    "            ('relu1_1', nn.ReLU()),\n",
    "            ('conv1_2',  nn.Conv2d(out_channel // 4, out_channel // 4, kernel_size=(3, 3), stride=stride, padding=1)),\n",
    "            ('bn1_2', nn.BatchNorm2d(out_channel // 4)),\n",
    "            ('relu1_2', nn.ReLU()),\n",
    "            ('conv1_3', nn.Conv2d(out_channel // 4, out_channel, kernel_size=(1, 1))),\n",
    "            ('bn1_3', nn.BatchNorm2d(out_channel)),\n",
    "\n",
    "        ]))\n",
    "        self.last_relu = nn.ReLU()\n",
    "        if stride != 1 or in_channels != out_channel:\n",
    "            self.shortcut = nn.Sequential(OrderedDict([\n",
    "                ('sho_conv_1', nn.Conv2d(in_channels, out_channel, kernel_size=(1, 1), stride=stride)),\n",
    "                ('sho_bn1', nn.BatchNorm2d(out_channel))\n",
    "            ]))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x) + self.shortcut(x)\n",
    "        x = self.last_relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1_1', nn.Conv2d(3, 64, kernel_size=(3, 3), stride=2)),\n",
    "            ('bn1_1', nn.BatchNorm2d(64)),\n",
    "            ('relu1_1', nn.ReLU()),\n",
    "            ('block1', BottleNeckBlock(in_channels=64, out_channel=256)),\n",
    "            ('block2', BottleNeckBlock(in_channels=256, out_channel=256)),\n",
    "            ('block3', BottleNeckBlock(in_channels=256, out_channel=256)),\n",
    "            ('block4', BottleNeckBlock(in_channels=256, out_channel=512, stride=2)),\n",
    "\n",
    "            ('block5', BottleNeckBlock(in_channels=512, out_channel=512)),\n",
    "            ('block6', BottleNeckBlock(in_channels=512, out_channel=512)),\n",
    "            ('block7', BottleNeckBlock(in_channels=512, out_channel=512)),\n",
    "            ('block8', BottleNeckBlock(in_channels=512, out_channel=1024, stride=2)),\n",
    "            \n",
    "            ('block9', BottleNeckBlock(in_channels=1024, out_channel=1024)),\n",
    "            ('block10', BottleNeckBlock(in_channels=1024, out_channel=1024)),\n",
    "            ('block11', BottleNeckBlock(in_channels=1024, out_channel=1024)),\n",
    "            ('block12', BottleNeckBlock(in_channels=1024, out_channel=1024)),\n",
    "            ('block13', BottleNeckBlock(in_channels=1024, out_channel=1024, stride=2)),\n",
    "\n",
    "            ('block14', BottleNeckBlock(in_channels=1024, out_channel=2048)),\n",
    "            ('block15', BottleNeckBlock(in_channels=2048, out_channel=2048)),\n",
    "            ('block16', BottleNeckBlock(in_channels=2048, out_channel=2048)),\n",
    "\n",
    "        ]))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(2048, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0d3852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n",
      "Starting Epoch 1\n",
      "Batch 0/44, Loss: 4.9940\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.7035\n",
      "Starting Epoch 2\n",
      "Batch 0/44, Loss: 4.5979\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.4422\n",
      "Epoch 2 finished\n",
      "average training loss is 4.4422\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.4422\n",
      "Validation - Loss: 4.3492\n",
      "Starting Epoch 3\n",
      "Batch 0/44, Loss: 4.2590\n",
      "Epoch 3 finished\n",
      "Training - Loss: 4.1690\n",
      "Starting Epoch 4\n",
      "Batch 0/44, Loss: 4.1240\n",
      "Epoch 4 finished\n",
      "Training - Loss: 4.0634\n",
      "Epoch 4 finished\n",
      "average training loss is 4.0634\n",
      "Epoch 4 finished\n",
      "Training - Loss: 4.0634\n",
      "Validation - Loss: 9.5068\n",
      "Starting Epoch 5\n",
      "Batch 0/44, Loss: 3.9288\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.9328\n",
      "Starting Epoch 6\n",
      "Batch 0/44, Loss: 3.8484\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.8032\n",
      "Epoch 6 finished\n",
      "average training loss is 3.8032\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.8032\n",
      "Validation - Loss: 3.7637\n",
      "Starting Epoch 7\n",
      "Batch 0/44, Loss: 3.7167\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.6773\n",
      "Starting Epoch 8\n",
      "Batch 0/44, Loss: 3.5989\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.5880\n",
      "Epoch 8 finished\n",
      "average training loss is 3.5880\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.5880\n",
      "Validation - Loss: 3.4896\n",
      "Starting Epoch 9\n",
      "Batch 0/44, Loss: 3.4985\n",
      "Epoch 9 finished\n",
      "Training - Loss: 3.4738\n",
      "Starting Epoch 10\n",
      "Batch 0/44, Loss: 3.4038\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.3601\n",
      "Epoch 10 finished\n",
      "average training loss is 3.3601\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.3601\n",
      "Validation - Loss: 3.2426\n",
      "Starting Epoch 11\n",
      "Batch 0/44, Loss: 3.2365\n",
      "Epoch 11 finished\n",
      "Training - Loss: 3.2322\n",
      "Starting Epoch 12\n",
      "Batch 0/44, Loss: 3.0808\n",
      "Epoch 12 finished\n",
      "Training - Loss: 3.1366\n",
      "Epoch 12 finished\n",
      "average training loss is 3.1366\n",
      "Epoch 12 finished\n",
      "Training - Loss: 3.1366\n",
      "Validation - Loss: 3.9901\n",
      "Starting Epoch 13\n",
      "Batch 0/44, Loss: 2.9780\n",
      "Epoch 13 finished\n",
      "Training - Loss: 3.1633\n",
      "Starting Epoch 14\n",
      "Batch 0/44, Loss: 3.0931\n",
      "Epoch 14 finished\n",
      "Training - Loss: 3.0235\n",
      "Epoch 14 finished\n",
      "average training loss is 3.0235\n",
      "Epoch 14 finished\n",
      "Training - Loss: 3.0235\n",
      "Validation - Loss: 2.8482\n",
      "Starting Epoch 15\n",
      "Batch 0/44, Loss: 2.9145\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.9637\n",
      "Starting Epoch 16\n",
      "Batch 0/44, Loss: 3.0651\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.9259\n",
      "Epoch 16 finished\n",
      "average training loss is 2.9259\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.9259\n",
      "Validation - Loss: 2.8847\n",
      "Starting Epoch 17\n",
      "Batch 0/44, Loss: 2.8281\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.7924\n",
      "Starting Epoch 18\n",
      "Batch 0/44, Loss: 2.6297\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.7286\n",
      "Epoch 18 finished\n",
      "average training loss is 2.7286\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.7286\n",
      "Validation - Loss: 2.7242\n",
      "Starting Epoch 19\n",
      "Batch 0/44, Loss: 2.5048\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.7327\n",
      "Starting Epoch 20\n",
      "Batch 0/44, Loss: 3.1269\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.8166\n",
      "Epoch 20 finished\n",
      "average training loss is 2.8166\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.8166\n",
      "Validation - Loss: 6.1177\n",
      "Starting Epoch 21\n",
      "Batch 0/44, Loss: 2.5717\n",
      "Epoch 21 finished\n",
      "Training - Loss: 2.8175\n",
      "Starting Epoch 22\n",
      "Batch 0/44, Loss: 2.9775\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.6595\n",
      "Epoch 22 finished\n",
      "average training loss is 2.6595\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.6595\n",
      "Validation - Loss: 2.5991\n",
      "Starting Epoch 23\n",
      "Batch 0/44, Loss: 2.4329\n",
      "Epoch 23 finished\n",
      "Training - Loss: 2.4701\n",
      "Starting Epoch 24\n",
      "Batch 0/44, Loss: 2.3694\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.3769\n",
      "Epoch 24 finished\n",
      "average training loss is 2.3769\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.3769\n",
      "Validation - Loss: 2.2986\n",
      "Starting Epoch 25\n",
      "Batch 0/44, Loss: 2.3219\n",
      "Epoch 25 finished\n",
      "Training - Loss: 2.3068\n",
      "Starting Epoch 26\n",
      "Batch 0/44, Loss: 2.1822\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.2521\n",
      "Epoch 26 finished\n",
      "average training loss is 2.2521\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.2521\n",
      "Validation - Loss: 2.4135\n",
      "Starting Epoch 27\n",
      "Batch 0/44, Loss: 2.2301\n",
      "Epoch 27 finished\n",
      "Training - Loss: 2.1842\n",
      "Starting Epoch 28\n",
      "Batch 0/44, Loss: 2.1313\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.1212\n",
      "Epoch 28 finished\n",
      "average training loss is 2.1212\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.1212\n",
      "Validation - Loss: 2.2511\n",
      "Starting Epoch 29\n",
      "Batch 0/44, Loss: 2.0741\n",
      "Epoch 29 finished\n",
      "Training - Loss: 2.0690\n",
      "Starting Epoch 30\n",
      "Batch 0/44, Loss: 2.0560\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.0170\n",
      "Epoch 30 finished\n",
      "average training loss is 2.0170\n",
      "Epoch 30 finished\n",
      "Training - Loss: 2.0170\n",
      "Validation - Loss: 2.1321\n",
      "Starting Epoch 31\n",
      "Batch 0/44, Loss: 2.0349\n",
      "Epoch 31 finished\n",
      "Training - Loss: 1.9531\n",
      "Starting Epoch 32\n",
      "Batch 0/44, Loss: 1.8577\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.9040\n",
      "Epoch 32 finished\n",
      "average training loss is 1.9040\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.9040\n",
      "Validation - Loss: 2.0439\n",
      "Starting Epoch 33\n",
      "Batch 0/44, Loss: 1.8461\n",
      "Epoch 33 finished\n",
      "Training - Loss: 1.8677\n",
      "Starting Epoch 34\n",
      "Batch 0/44, Loss: 1.8072\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.8155\n",
      "Epoch 34 finished\n",
      "average training loss is 1.8155\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.8155\n",
      "Validation - Loss: 1.9808\n",
      "Starting Epoch 35\n",
      "Batch 0/44, Loss: 1.7676\n",
      "Epoch 35 finished\n",
      "Training - Loss: 1.7755\n",
      "Starting Epoch 36\n",
      "Batch 0/44, Loss: 1.7526\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.7328\n",
      "Epoch 36 finished\n",
      "average training loss is 1.7328\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.7328\n",
      "Validation - Loss: 1.9709\n",
      "Starting Epoch 37\n",
      "Batch 0/44, Loss: 1.7050\n",
      "Epoch 37 finished\n",
      "Training - Loss: 1.7089\n",
      "Starting Epoch 38\n",
      "Batch 0/44, Loss: 1.6621\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.6843\n",
      "Epoch 38 finished\n",
      "average training loss is 1.6843\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.6843\n",
      "Validation - Loss: 1.9521\n",
      "Starting Epoch 39\n",
      "Batch 0/44, Loss: 1.6632\n",
      "Epoch 39 finished\n",
      "Training - Loss: 1.6698\n",
      "Starting Epoch 40\n",
      "Batch 0/44, Loss: 1.6699\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.6659\n",
      "Epoch 40 finished\n",
      "average training loss is 1.6659\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.6659\n",
      "Validation - Loss: 1.9654\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.7)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "resnet = ResNet50().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    resnet.parameters(),\n",
    "    lr=3e-3,  # Keep this for now, let OneCycleLR handle it\n",
    "    weight_decay=2e-3\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=4e-3,                # Slightly lower peak LR for stability\n",
    "    epochs=num_epochs,          # Keep 45 epochs\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.4,              # Increase warmup to 40% (18 epochs)\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=12.0,            # Start LR = 5e-3 / 12 = 4.2e-4\n",
    "    final_div_factor=400.0      # Final LR = 5e-3 / 400 = 1.25e-5\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    resnet.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = resnet(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        resnet.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = resnet(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbc7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 63.50%\n",
      "Standard Test Accuracy: 63.5000 (63.50%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(resnet)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
