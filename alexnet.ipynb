{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d1ba1d",
   "metadata": {},
   "source": [
    "Implement AlexNet using PyTorch.\n",
    "\n",
    "Link: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "5 convolutional and 3 fully connected layers\n",
    "\n",
    "Use ReLU activations after each convolutional and fully connected layer.\n",
    "\n",
    "ReLUs have the desirable property that they do not require input normalization to prevent them\n",
    "from saturating.\n",
    "\n",
    "Uses overlapping pooling (i.e. pooling windows that overlap).\n",
    "\n",
    "Mistakes:\n",
    "\n",
    "Missing flatten: After conv layers, you need to flatten before the first linear layer\n",
    "\n",
    "Wrong input size to linear layer: After the conv layers, the spatial dimensions aren't 1x1, so input to fcl_1 should be calculated, not just 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        # After self.layers definition, add:\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 3, 32, 32)\n",
    "            out = self.layers(x)\n",
    "            self.flattened_size = out.view(1, -1).size(1)\n",
    "\n",
    "        self.fcl_1 = nn.Linear(self.flattened_size, 2048)        \n",
    "        self.fcl_2 = nn.Linear(2048, 2048)\n",
    "        self.fcl_3 = nn.Linear(2048, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcl_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fcl_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fcl_3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.25)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "model = AlexNet().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,  # Lower initial LR\n",
    "    weight_decay=1e-4  # Reduce weight decay\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-3,  # Much lower peak (was 5e-3)\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,  # Shorter warmup\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,  # Gentler start\n",
    "    final_div_factor=1000.0\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ADD THIS\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(model)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
