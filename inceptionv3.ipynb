{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c250568",
   "metadata": {},
   "source": [
    "Paper link: https://arxiv.org/pdf/1512.00567\n",
    "\n",
    "Problem: Although VGGNet has the compelling feature of\n",
    "architectural simplicity, this comes at a high cost: evaluating the network requires a lot of computation. \n",
    "\n",
    "The original motivation was to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combating the vanishing gradient problem in very deep networks.\n",
    "\n",
    "The sources explain that a standard n×n convolution can be factorized into two successive asymmetric convolutions.\n",
    "\n",
    "Sliding a 3×1 convolution followed by a 1×3 convolution results in the same receptive field as a single 3×3 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79ae96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class InceptionBlock1(nn.Module):\n",
    "    def __init__(self, in_channels, ch1, ch2, ch3, ch4):\n",
    "        \"\"\"\n",
    "        ch1, ch2, ch3, ch4 = output channels for each of the 4 paths\n",
    "        Total output channels = ch1 + ch2 + ch3 + ch4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1//2, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//2, ch1//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//2, ch1, kernel_size=3, padding=1),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch2, kernel_size=1, padding=0),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(in_channels, ch3, kernel_size=1),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch4//2, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch4//2, ch4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.path1(x), self.path2(x), self.path3(x), self.path4(x)], dim=1)\n",
    "\n",
    "class InceptionBlock2(nn.Module):\n",
    "    def __init__(self, in_channels, ch1, ch2, ch3, ch4, kernel_size=7):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1//4, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//4, ch1//4, kernel_size=(1, kernel_size), padding=(0, pad)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//4, ch1//4, kernel_size=(kernel_size, 1), padding=(pad, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//4, ch1//4, kernel_size=(1, kernel_size), padding=(0, pad)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch1//4, ch1, kernel_size=(kernel_size, 1), padding=(pad, 0)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch2, kernel_size=1, padding=0),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(in_channels, ch3, kernel_size=1),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch4//2, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch4//2, ch4//2, kernel_size=(1, kernel_size), padding=(0, pad)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch4//2, ch4, kernel_size=(kernel_size, 1), padding=(pad, 0)),\n",
    "            nn.ReLU(),                \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.path1(x), self.path2(x), self.path3(x), self.path4(x)], dim=1)\n",
    "\n",
    "class InceptionBlock3(nn.Module):\n",
    "    def __init__(self, in_channels, ch1, ch2, ch3, ch4, ch5, ch6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1, kernel_size=1, padding=0),\n",
    "            nn.ReLU()             \n",
    "        )\n",
    "\n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(in_channels, ch2, kernel_size=1),\n",
    "            nn.ReLU()            \n",
    "        )\n",
    "\n",
    "        self.path3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3//2, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch3//2, ch3, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.ReLU(),                        \n",
    "        )\n",
    "\n",
    "        self.path4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch4//2, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch4//2, ch4, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.ReLU(),                        \n",
    "        )\n",
    "\n",
    "        self.path5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5//4, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5//4, ch5//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5//2, ch5, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        self.path6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch6//4, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch6//4, ch6//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch6//2, ch6, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.ReLU(), \n",
    "        )   \n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.path1(x), self.path2(x), self.path3(x), \n",
    "                         self.path4(x), self.path5(x), self.path6(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc4d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # Stem (same as yours)\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 80, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(80),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(80, 192, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(192, 288, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(288),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            # Inception blocks with exact channel control\n",
    "            # Example: Want 288 total output? Use ch1=64, ch2=64, ch3=80, ch4=80 → 288\n",
    "            InceptionBlock1(192, ch1=64, ch2=64, ch3=80, ch4=80),     # Output: 288\n",
    "            InceptionBlock2(288, ch1=96, ch2=96, ch3=96, ch4=96),     # Output: 384\n",
    "            InceptionBlock3(384, ch1=64, ch2=64, ch3=64, ch4=64, ch5=64, ch6=64),  # Output: 384\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Changed to (1,1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(384, 100)  # Match last block output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007560c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n",
      "Starting Epoch 1\n",
      "Batch 0/88, Loss: 4.6075\n",
      "Batch 50/88, Loss: 4.3889\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.4297\n",
      "Starting Epoch 2\n",
      "Batch 0/88, Loss: 4.2225\n",
      "Batch 50/88, Loss: 3.9996\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0662\n",
      "Epoch 2 finished\n",
      "average training loss is 4.0662\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0662\n",
      "Validation - Loss: 3.8655\n",
      "Starting Epoch 3\n",
      "Batch 0/88, Loss: 3.8620\n",
      "Batch 50/88, Loss: 3.8129\n",
      "Epoch 3 finished\n",
      "Training - Loss: 3.8815\n",
      "Starting Epoch 4\n",
      "Batch 0/88, Loss: 3.7847\n",
      "Batch 50/88, Loss: 3.7783\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.7461\n",
      "Epoch 4 finished\n",
      "average training loss is 3.7461\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.7461\n",
      "Validation - Loss: 3.6653\n",
      "Starting Epoch 5\n",
      "Batch 0/88, Loss: 3.5691\n",
      "Batch 50/88, Loss: 3.6544\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.6142\n",
      "Starting Epoch 6\n",
      "Batch 0/88, Loss: 3.5104\n",
      "Batch 50/88, Loss: 3.4136\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.4690\n",
      "Epoch 6 finished\n",
      "average training loss is 3.4690\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.4690\n",
      "Validation - Loss: 3.5243\n",
      "Starting Epoch 7\n",
      "Batch 0/88, Loss: 3.3731\n",
      "Batch 50/88, Loss: 3.2816\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.3638\n",
      "Starting Epoch 8\n",
      "Batch 0/88, Loss: 3.2338\n",
      "Batch 50/88, Loss: 3.2512\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.2481\n",
      "Epoch 8 finished\n",
      "average training loss is 3.2481\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.2481\n",
      "Validation - Loss: 3.2103\n",
      "Starting Epoch 9\n",
      "Batch 0/88, Loss: 3.2742\n",
      "Batch 50/88, Loss: 3.2014\n",
      "Epoch 9 finished\n",
      "Training - Loss: 3.1559\n",
      "Starting Epoch 10\n",
      "Batch 0/88, Loss: 3.0775\n",
      "Batch 50/88, Loss: 3.0392\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0611\n",
      "Epoch 10 finished\n",
      "average training loss is 3.0611\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0611\n",
      "Validation - Loss: 2.9974\n",
      "Starting Epoch 11\n",
      "Batch 0/88, Loss: 2.9898\n",
      "Batch 50/88, Loss: 3.0579\n",
      "Epoch 11 finished\n",
      "Training - Loss: 2.9743\n",
      "Starting Epoch 12\n",
      "Batch 0/88, Loss: 2.9299\n",
      "Batch 50/88, Loss: 2.9306\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.9102\n",
      "Epoch 12 finished\n",
      "average training loss is 2.9102\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.9102\n",
      "Validation - Loss: 2.8544\n",
      "Starting Epoch 13\n",
      "Batch 0/88, Loss: 2.8827\n",
      "Batch 50/88, Loss: 2.8003\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.8320\n",
      "Starting Epoch 14\n",
      "Batch 0/88, Loss: 2.8535\n",
      "Batch 50/88, Loss: 2.7899\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.7636\n",
      "Epoch 14 finished\n",
      "average training loss is 2.7636\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.7636\n",
      "Validation - Loss: 2.6080\n",
      "Starting Epoch 15\n",
      "Batch 0/88, Loss: 2.6854\n",
      "Batch 50/88, Loss: 2.5910\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.6948\n",
      "Starting Epoch 16\n",
      "Batch 0/88, Loss: 2.6732\n",
      "Batch 50/88, Loss: 2.5935\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.6403\n",
      "Epoch 16 finished\n",
      "average training loss is 2.6403\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.6403\n",
      "Validation - Loss: 2.4946\n",
      "Starting Epoch 17\n",
      "Batch 0/88, Loss: 2.6305\n",
      "Batch 50/88, Loss: 2.5304\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.5844\n",
      "Starting Epoch 18\n",
      "Batch 0/88, Loss: 2.5698\n",
      "Batch 50/88, Loss: 2.6087\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.5452\n",
      "Epoch 18 finished\n",
      "average training loss is 2.5452\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.5452\n",
      "Validation - Loss: 2.4458\n",
      "Starting Epoch 19\n",
      "Batch 0/88, Loss: 2.5984\n",
      "Batch 50/88, Loss: 2.5492\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.5300\n",
      "Starting Epoch 20\n",
      "Batch 0/88, Loss: 2.5558\n",
      "Batch 50/88, Loss: 2.4891\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5156\n",
      "Epoch 20 finished\n",
      "average training loss is 2.5156\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5156\n",
      "Validation - Loss: 2.4293\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.25)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,\n",
    "    batch_size=512,  # Changed from 512\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "model = InceptionV3().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-3,  # Changed from 1e-3\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-2,  # Changed from 3e-3\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1000.0\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Changed from 1.0\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a62045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 49.25%\n",
      "Standard Test Accuracy: 49.2500 (49.25%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(model)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5757e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
