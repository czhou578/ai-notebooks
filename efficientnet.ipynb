{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bd4791",
   "metadata": {},
   "source": [
    "EfficientNet Architecture (Paper link: https://arxiv.org/pdf/1905.11946)\n",
    "\n",
    "- MBConv Block Architecture:\n",
    "1x1 Conv\n",
    "Batchnorm\n",
    "relu\n",
    "grouped conv\n",
    "bnorm\n",
    "relu\n",
    "sqz and excite\n",
    "1x1 conv\n",
    "batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ae976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor, reduction_factor = 4, stride = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        expanded_channels = in_channels * expansion_factor\n",
    "\n",
    "        self.use_skip = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.expansion_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, expanded_channels, kernel_size=(1, 1), stride=1),\n",
    "            nn.BatchNorm2d(expanded_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(expanded_channels, expanded_channels, kernel_size=(3, 3), groups=expanded_channels, padding=1, stride=stride),\n",
    "            nn.BatchNorm2d(expanded_channels),\n",
    "            nn.SiLU(),        \n",
    "        )\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.excitation_layers = nn.Sequential(\n",
    "            nn.Linear(expanded_channels, expanded_channels // reduction_factor),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(expanded_channels // reduction_factor, expanded_channels),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.contract_layers = nn.Sequential(\n",
    "            nn.Conv2d(expanded_channels, out_channels, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.expansion_layers(x)\n",
    "        pooled_features = self.global_avg_pool(features)\n",
    "        squeezed_features = pooled_features.squeeze(-1).squeeze(-1)\n",
    "        excited_features = self.excitation_layers(squeezed_features)\n",
    "        unsqueezed_features = excited_features.unsqueeze(-1).unsqueeze(-1)\n",
    "        attention_features = unsqueezed_features * features\n",
    "        final_features = self.contract_layers(attention_features)\n",
    "\n",
    "        return final_features + x if self.use_skip else final_features\n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, stride=1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            MBConvBlock(32, 64, 2, 4, 2),\n",
    "            MBConvBlock(64, 64, 2, 4),\n",
    "            MBConvBlock(64, 64, 2, 4),\n",
    "\n",
    "            MBConvBlock(64, 128, 3, 4, 2),\n",
    "            MBConvBlock(128, 128, 3, 4, 1),\n",
    "            MBConvBlock(128, 128, 3, 4, 1),\n",
    "            MBConvBlock(128, 128, 3, 4, 1),\n",
    "\n",
    "            MBConvBlock(128, 256, 3, 4, 2),\n",
    "            MBConvBlock(256, 256, 3, 4, 1),\n",
    "            MBConvBlock(256, 256, 3, 4, 1),\n",
    "            MBConvBlock(256, 256, 3, 4, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(512, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        x = self.dropout(self.linear(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e98a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 15.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:13<00:00, 12.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Batch 0/44, Loss: 4.6644\n",
      "Epoch 1 finished\n",
      "Training - Loss: 4.3726\n",
      "Starting Epoch 2\n",
      "Batch 0/44, Loss: 4.1867\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0882\n",
      "Epoch 2 finished\n",
      "average training loss is 4.0882\n",
      "Epoch 2 finished\n",
      "Training - Loss: 4.0882\n",
      "Validation - Loss: 3.7209\n",
      "Starting Epoch 3\n",
      "Batch 0/44, Loss: 3.9118\n",
      "Epoch 3 finished\n",
      "Training - Loss: 3.8965\n",
      "Starting Epoch 4\n",
      "Batch 0/44, Loss: 3.8041\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.7362\n",
      "Epoch 4 finished\n",
      "average training loss is 3.7362\n",
      "Epoch 4 finished\n",
      "Training - Loss: 3.7362\n",
      "Validation - Loss: 3.3200\n",
      "Starting Epoch 5\n",
      "Batch 0/44, Loss: 3.5924\n",
      "Epoch 5 finished\n",
      "Training - Loss: 3.5829\n",
      "Starting Epoch 6\n",
      "Batch 0/44, Loss: 3.5447\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.4565\n",
      "Epoch 6 finished\n",
      "average training loss is 3.4565\n",
      "Epoch 6 finished\n",
      "Training - Loss: 3.4565\n",
      "Validation - Loss: 3.0325\n",
      "Starting Epoch 7\n",
      "Batch 0/44, Loss: 3.2692\n",
      "Epoch 7 finished\n",
      "Training - Loss: 3.3411\n",
      "Starting Epoch 8\n",
      "Batch 0/44, Loss: 3.2773\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.2628\n",
      "Epoch 8 finished\n",
      "average training loss is 3.2628\n",
      "Epoch 8 finished\n",
      "Training - Loss: 3.2628\n",
      "Validation - Loss: 2.8547\n",
      "Starting Epoch 9\n",
      "Batch 0/44, Loss: 3.1767\n",
      "Epoch 9 finished\n",
      "Training - Loss: 3.1649\n",
      "Starting Epoch 10\n",
      "Batch 0/44, Loss: 3.0751\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0868\n",
      "Epoch 10 finished\n",
      "average training loss is 3.0868\n",
      "Epoch 10 finished\n",
      "Training - Loss: 3.0868\n",
      "Validation - Loss: 2.6442\n",
      "Starting Epoch 11\n",
      "Batch 0/44, Loss: 3.0790\n",
      "Epoch 11 finished\n",
      "Training - Loss: 3.0240\n",
      "Starting Epoch 12\n",
      "Batch 0/44, Loss: 2.9590\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.9517\n",
      "Epoch 12 finished\n",
      "average training loss is 2.9517\n",
      "Epoch 12 finished\n",
      "Training - Loss: 2.9517\n",
      "Validation - Loss: 2.5948\n",
      "Starting Epoch 13\n",
      "Batch 0/44, Loss: 2.8713\n",
      "Epoch 13 finished\n",
      "Training - Loss: 2.9016\n",
      "Starting Epoch 14\n",
      "Batch 0/44, Loss: 2.8387\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.8369\n",
      "Epoch 14 finished\n",
      "average training loss is 2.8369\n",
      "Epoch 14 finished\n",
      "Training - Loss: 2.8369\n",
      "Validation - Loss: 2.4414\n",
      "Starting Epoch 15\n",
      "Batch 0/44, Loss: 2.7206\n",
      "Epoch 15 finished\n",
      "Training - Loss: 2.7777\n",
      "Starting Epoch 16\n",
      "Batch 0/44, Loss: 2.6599\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.7208\n",
      "Epoch 16 finished\n",
      "average training loss is 2.7208\n",
      "Epoch 16 finished\n",
      "Training - Loss: 2.7208\n",
      "Validation - Loss: 2.3700\n",
      "Starting Epoch 17\n",
      "Batch 0/44, Loss: 2.6219\n",
      "Epoch 17 finished\n",
      "Training - Loss: 2.6645\n",
      "Starting Epoch 18\n",
      "Batch 0/44, Loss: 2.5579\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.6096\n",
      "Epoch 18 finished\n",
      "average training loss is 2.6096\n",
      "Epoch 18 finished\n",
      "Training - Loss: 2.6096\n",
      "Validation - Loss: 2.1566\n",
      "Starting Epoch 19\n",
      "Batch 0/44, Loss: 2.5647\n",
      "Epoch 19 finished\n",
      "Training - Loss: 2.5495\n",
      "Starting Epoch 20\n",
      "Batch 0/44, Loss: 2.4337\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5066\n",
      "Epoch 20 finished\n",
      "average training loss is 2.5066\n",
      "Epoch 20 finished\n",
      "Training - Loss: 2.5066\n",
      "Validation - Loss: 2.1545\n",
      "Starting Epoch 21\n",
      "Batch 0/44, Loss: 2.4238\n",
      "Epoch 21 finished\n",
      "Training - Loss: 2.4513\n",
      "Starting Epoch 22\n",
      "Batch 0/44, Loss: 2.3183\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.3869\n",
      "Epoch 22 finished\n",
      "average training loss is 2.3869\n",
      "Epoch 22 finished\n",
      "Training - Loss: 2.3869\n",
      "Validation - Loss: 2.0382\n",
      "Starting Epoch 23\n",
      "Batch 0/44, Loss: 2.2819\n",
      "Epoch 23 finished\n",
      "Training - Loss: 2.3445\n",
      "Starting Epoch 24\n",
      "Batch 0/44, Loss: 2.2976\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.2915\n",
      "Epoch 24 finished\n",
      "average training loss is 2.2915\n",
      "Epoch 24 finished\n",
      "Training - Loss: 2.2915\n",
      "Validation - Loss: 1.9834\n",
      "Starting Epoch 25\n",
      "Batch 0/44, Loss: 2.2816\n",
      "Epoch 25 finished\n",
      "Training - Loss: 2.2517\n",
      "Starting Epoch 26\n",
      "Batch 0/44, Loss: 2.1827\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.1914\n",
      "Epoch 26 finished\n",
      "average training loss is 2.1914\n",
      "Epoch 26 finished\n",
      "Training - Loss: 2.1914\n",
      "Validation - Loss: 2.0000\n",
      "Starting Epoch 27\n",
      "Batch 0/44, Loss: 2.1711\n",
      "Epoch 27 finished\n",
      "Training - Loss: 2.1405\n",
      "Starting Epoch 28\n",
      "Batch 0/44, Loss: 2.1030\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.0929\n",
      "Epoch 28 finished\n",
      "average training loss is 2.0929\n",
      "Epoch 28 finished\n",
      "Training - Loss: 2.0929\n",
      "Validation - Loss: 1.8840\n",
      "Starting Epoch 29\n",
      "Batch 0/44, Loss: 2.0156\n",
      "Epoch 29 finished\n",
      "Training - Loss: 2.0458\n",
      "Starting Epoch 30\n",
      "Batch 0/44, Loss: 1.9628\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.9912\n",
      "Epoch 30 finished\n",
      "average training loss is 1.9912\n",
      "Epoch 30 finished\n",
      "Training - Loss: 1.9912\n",
      "Validation - Loss: 1.8548\n",
      "Starting Epoch 31\n",
      "Batch 0/44, Loss: 1.9573\n",
      "Epoch 31 finished\n",
      "Training - Loss: 1.9576\n",
      "Starting Epoch 32\n",
      "Batch 0/44, Loss: 1.9612\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.9049\n",
      "Epoch 32 finished\n",
      "average training loss is 1.9049\n",
      "Epoch 32 finished\n",
      "Training - Loss: 1.9049\n",
      "Validation - Loss: 1.8269\n",
      "Starting Epoch 33\n",
      "Batch 0/44, Loss: 1.9142\n",
      "Epoch 33 finished\n",
      "Training - Loss: 1.8769\n",
      "Starting Epoch 34\n",
      "Batch 0/44, Loss: 1.8206\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.8407\n",
      "Epoch 34 finished\n",
      "average training loss is 1.8407\n",
      "Epoch 34 finished\n",
      "Training - Loss: 1.8407\n",
      "Validation - Loss: 1.8242\n",
      "Starting Epoch 35\n",
      "Batch 0/44, Loss: 1.7739\n",
      "Epoch 35 finished\n",
      "Training - Loss: 1.8118\n",
      "Starting Epoch 36\n",
      "Batch 0/44, Loss: 1.7535\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.7964\n",
      "Epoch 36 finished\n",
      "average training loss is 1.7964\n",
      "Epoch 36 finished\n",
      "Training - Loss: 1.7964\n",
      "Validation - Loss: 1.8122\n",
      "Starting Epoch 37\n",
      "Batch 0/44, Loss: 1.7717\n",
      "Epoch 37 finished\n",
      "Training - Loss: 1.7901\n",
      "Starting Epoch 38\n",
      "Batch 0/44, Loss: 1.8222\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.7720\n",
      "Epoch 38 finished\n",
      "average training loss is 1.7720\n",
      "Epoch 38 finished\n",
      "Training - Loss: 1.7720\n",
      "Validation - Loss: 1.8076\n",
      "Starting Epoch 39\n",
      "Batch 0/44, Loss: 1.6900\n",
      "Epoch 39 finished\n",
      "Training - Loss: 1.7668\n",
      "Starting Epoch 40\n",
      "Batch 0/44, Loss: 1.8154\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.7699\n",
      "Epoch 40 finished\n",
      "average training loss is 1.7699\n",
      "Epoch 40 finished\n",
      "Training - Loss: 1.7699\n",
      "Validation - Loss: 1.8064\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Mild to avoid over-distortion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]),\n",
    "    transforms.RandomErasing(p=0.5)  # Apply after normalization for consistency\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Moved ToTensor before Normalize (good practice)\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "# Load raw datasets\n",
    "cifar_train_raw = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=None)\n",
    "\n",
    "train_size = int(0.9 * len(cifar_train_raw))  # 48,000\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(cifar_train_raw)))\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "cifar_train = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "cifar_val = Subset(\n",
    "    datasets.CIFAR100(root=\"./data\", train=True, transform=test_transform),\n",
    "    val_indices\n",
    ")\n",
    "# Use original test set (10,000 samples) - close to 10% of 60,000\n",
    "cifar_test = datasets.CIFAR100(root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cifar_train,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cifar_val,  # Use directly\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar_test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "model = EfficientNet().to(device)\n",
    "\n",
    "num_epochs = 40\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "base_lr = 4e-3\n",
    "\n",
    "batch_scale = 1024 / 256  # 4x larger batches\n",
    "scaled_lr = base_lr * batch_scale**0.5  # Square root scaling\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-3,  # Keep this for now, let OneCycleLR handle it\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-3,                # Slightly lower peak LR for stability\n",
    "    epochs=num_epochs,          # Keep 45 epochs\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.4,              # Increase warmup to 40% (18 epochs)\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=12.0,            # Start LR = 5e-3 / 12 = 4.2e-4\n",
    "    final_div_factor=400.0      # Final LR = 5e-3 / 400 = 1.25e-5\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    avg_train_loss = current_loss / num_batches\n",
    "    print(f'Epoch {epoch+1} finished')\n",
    "    print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'average training loss is {avg_train_loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)  # Convert inputs to FP16\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = loss_function(val_outputs, val_targets)\n",
    "\n",
    "                val_loss += val_batch_loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running standard evaluation ===\n",
      "Starting evaluation...\n",
      "Accuracy of the network on the test images: 70.43%\n",
      "Standard Test Accuracy: 70.4300 (7043.00%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use autocast for consistency if you trained with it\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\n=== Running standard evaluation ===\")\n",
    "standard_accuracy = evaluate_test_set(model)   \n",
    "print(f'Standard Test Accuracy: {standard_accuracy:.4f} ({standard_accuracy*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
