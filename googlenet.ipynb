{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab719679",
   "metadata": {},
   "source": [
    "Paper link: https://arxiv.org/pdf/1409.4842\n",
    "\n",
    "Problem: Can we make networks more sparse while still being expressive and learnable? Fully connected layers are expensive and prone to overfitting.\n",
    "\n",
    "Sparse connections: Each neuron only connects to a small subset of neurons in the previous layer\n",
    "\n",
    "If two neurons are frequently active together, they are likely:\n",
    "- Responding to the same underlying cause\n",
    "- Inputs to the same higher-level feature\n",
    "\n",
    "1. Compute correlations between neurons in layer L\n",
    "2. Cluster neurons based on correlation\n",
    "3. Connect each neuron in layer L+1 to a cluster of neurons in layer L.\n",
    "\n",
    "This creates a sparse connectivity pattern that reflects the data distribution.\n",
    "Shared dependencies induce correlations between neurons.\n",
    "\n",
    "A lot of clusters can be concentrated in a small region, can be convered by 1x1 conv in the L + 1 layer.\n",
    "\n",
    "However, one can also expect that there will be a smaller number of more\n",
    "spatially spread out clusters that can be covered by convolutions over larger patches, and there\n",
    "will be a decreasing number of patches over larger and larger regions\n",
    "\n",
    "The reason why we use larger kernels as we move down is because we want larger receptive fields \n",
    "and reduced need for precise localization. As features become more abstract, we want each neuron to \n",
    "cover a larger area. (The ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.)\n",
    "\n",
    "Insert 1x1 conv layers to reduce the channel dimension. This preserves expressive power, dramatically reduces the number of parameters.\n",
    "\n",
    "- All the convolutions, including those inside the Inception modules, use rectified linear activation.\n",
    "\n",
    "“the strong performance of relatively shallower networks … suggests that the features produced by the layers in the middle of the network should be very discriminative”\n",
    "\n",
    "- implies that mid-level features (edges → textures → parts) are already quite useful for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class Path1(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45902908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
